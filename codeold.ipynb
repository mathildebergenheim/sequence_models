{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc95fee2910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import os\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "#from utils import train, set_device, compute_accuracy\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hjelpefunksjonene fra utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_device(device=None):\n",
    "    \"\"\"\n",
    "    Helper function to set device\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = (\n",
    "            torch.device('cuda') if torch.cuda.is_available()\n",
    "            else torch.device('cpu'))\n",
    "        print(f\"On device {device}.\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, val_loader=None, device=None):\n",
    "    device = set_device(device)\n",
    "\n",
    "    n_batch_train = len(train_loader)\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        loss_train = 0.0\n",
    "        for contexts, targets in train_loader:\n",
    "\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch_train)\n",
    "\n",
    "        if val_loader is not None:\n",
    "            loss_val = 0.0\n",
    "            n_batch_val = len(val_loader)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for contexts, targets in val_loader:\n",
    "                    contexts = contexts.to(device=device)\n",
    "                    targets = targets.to(device=device)\n",
    "                    outputs = model(contexts)\n",
    "                    loss_val += loss_fn(outputs, targets).item()\n",
    "            losses_val.append(loss_val / n_batch_val)\n",
    "            model.train()\n",
    "\n",
    "        print('{}  |  Epoch {}  |  Training loss {:.5f} | Validation loss {:.5f}'.format(\n",
    "            datetime.now().time(), epoch, losses_train[-1], losses_val[-1] if losses_val else 0.0))\n",
    "\n",
    "    # plot the loss graphs\n",
    "    plt.plot(losses_train, label='Training loss')\n",
    "    if losses_val:\n",
    "        plt.plot(losses_val, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return losses_train, losses_val\n",
    "\n",
    "\n",
    "def compute_accuracy(model, loader, device=None):\n",
    "    model.eval()\n",
    "    device = set_device(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += len(targets)\n",
    "            correct += int((predicted == targets).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    return acc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Word embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.1 and 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer will split a long text into a list of english words\n",
    "TOKENIZER_EN = get_tokenizer('basic_english')\n",
    "# Where we will store / load all our models, datasets, vocabulary, etc.\n",
    "PATH_GENERATED = './generated/'\n",
    "# Minimum number of occurence of a word in the text to add it to the vocabulary\n",
    "MIN_FREQ = 100\n",
    "\n",
    "def read_files(datapath='./data_train/'):\n",
    "    \"\"\"\n",
    "    Return a list of strings, one for each line in each .txt files in 'datapath'\n",
    "    \"\"\"\n",
    "    # Find all txt files in directory \n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith(\".txt\")]\n",
    "    \n",
    "    # Stores each line of each book in a list\n",
    "    lines = []\n",
    "    for f_name in files:\n",
    "        with open(f_name) as f:\n",
    "            lines += f.readlines()\n",
    "    return lines\n",
    "\n",
    "def tokenize(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Tokenize the list of lines\n",
    "    \"\"\"\n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "    return list_text\n",
    "\n",
    "def yield_tokens(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Yield tokens, ignoring names and digits to build vocabulary\n",
    "    \"\"\"\n",
    "    # Match any word containing digit\n",
    "    no_digits = '\\w*[0-9]+\\w*'\n",
    "    # Match word containing a uppercase \n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    # Match any sequence containing more than one space\n",
    "    no_spaces = '\\s+'\n",
    "    \n",
    "    for line in lines:\n",
    "        line = re.sub(no_digits, ' ', line)\n",
    "        line = re.sub(no_names, ' ', line)\n",
    "        line = re.sub(no_spaces, ' ', line)\n",
    "        yield tokenizer(line)\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    \"\"\"\n",
    "    Count occurrences of each word in vocabulary in the data\n",
    "    \n",
    "    Useful to get some insight on the data and to compute loss weights\n",
    "    \"\"\"\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    # vocab contains the vocabulary found in the data, associating an index to each word\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    # Since we removed all words with an uppercase when building the vocabulary, we skipped the word \"I\"\n",
    "    vocab.append_token(\"i\")\n",
    "    # Value of default index. This index will be returned when OOV (Out Of Vocabulary) token is queried.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:      2684706\n",
      "Total number of words in the validation dataset:    49526\n",
      "Total number of words in the test dataset:          124152\n",
      "Number of distinct words in the training dataset:   52105\n",
      "Number of distinct words kept (vocabulary size):    1880\n"
     ]
    }
   ],
   "source": [
    "line_books_train = []\n",
    "line_books_val = []\n",
    "line_books_test = []\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + \"words_train.pt\"):\n",
    "    words_train = torch.load(PATH_GENERATED + \"words_train.pt\")\n",
    "    words_val = torch.load(PATH_GENERATED + \"words_val.pt\")\n",
    "    words_test = torch.load(PATH_GENERATED + \"words_test.pt\")\n",
    "else:\n",
    "    # Get lists of strings, one for each line in each .txt files in 'datapath' \n",
    "    lines_books_train = read_files('data_train/')\n",
    "    lines_books_val = read_files('data_val/')\n",
    "    lines_books_test = read_files('data_test/')\n",
    "\n",
    "    # List of words contained in the dataset\n",
    "    words_train = tokenize(lines_books_train)\n",
    "    words_val = tokenize(lines_books_val)\n",
    "    words_test = tokenize(lines_books_test)\n",
    "    \n",
    "    torch.save(words_train , PATH_GENERATED + \"words_train.pt\")\n",
    "    torch.save(words_val , PATH_GENERATED + \"words_val.pt\")\n",
    "    torch.save(words_test , PATH_GENERATED + \"words_test.pt\")\n",
    "\n",
    "\n",
    "# ----------------------- Create vocabulary ----------------------------\n",
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "# Load vocabulary if you have already generated it\n",
    "# Otherwise, create it and save it\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    vocab = torch.load(PATH_GENERATED + VOCAB_FNAME)\n",
    "else:\n",
    "    # Create vocabulary based on the words in the training dataset\n",
    "    vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "    torch.save(vocab, PATH_GENERATED + VOCAB_FNAME)\n",
    "    \n",
    "\n",
    "# ------------------------ Quick analysis ------------------------------\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(\"Total number of words in the training dataset:     \", len(words_train))\n",
    "print(\"Total number of words in the validation dataset:   \", len(words_val))\n",
    "print(\"Total number of words in the test dataset:         \", len(words_test))\n",
    "print(\"Number of distinct words in the training dataset:  \", len(set(words_train)))\n",
    "print(\"Number of distinct words kept (vocabulary size):   \", VOCAB_SIZE)\n",
    "\n",
    "\n",
    "# GJØR DEN RASKERE \n",
    "#freqs = count_freqs(words_train, vocab)\n",
    "#print(\"occurences:\\n\", [(f.item(), w) for (f, w)  in zip(freqs, vocab.lookup_tokens(range(VOCAB_SIZE)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Define targets ------------------------------\n",
    "def compute_label(w):\n",
    "    \"\"\"\n",
    "    helper function to define MAP_TARGET\n",
    "    \n",
    "    - 0 = 'unknown word'\n",
    "    - 1 = 'punctuation' (i.e. the '<unk>' token)\n",
    "    - 2 = 'is an actual word'\n",
    "    \"\"\"\n",
    "    if w in ['<unk>']:\n",
    "        return 0\n",
    "    elif w in [',', '.', '(', ')', '?', '!']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# true labels for this task:\n",
    "MAP_TARGET = {\n",
    "    vocab[w]:compute_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))\n",
    "}\n",
    "\n",
    "# context size for this task \n",
    "CONTEXT_SIZE = 3\n",
    "\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def create_dataset(\n",
    "    text, vocab, \n",
    "    context_size=CONTEXT_SIZE, map_target=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    n_text = len(text)\n",
    "    n_vocab = len(vocab)\n",
    "    \n",
    "    # Change labels if only a few target are kept, otherwise, each word is\n",
    "    # associated with its index in the vocabulary\n",
    "    if map_target is None:\n",
    "        map_target = {i:i for i in range(n_vocab)}\n",
    "    \n",
    "    # Transform the text as a list of integers.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    # Start constructing the context / target pairs...\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - 2*context_size):\n",
    "        \n",
    "        # Word used to define target\n",
    "        t = txt[i + context_size]\n",
    "        \n",
    "        # Context before the target and after the target. \n",
    "        c = txt[i: i+context_size]\n",
    "        c += txt[i+context_size+1 : i + (2*context_size)+1]\n",
    "        \n",
    "        if compute_label(vocab.lookup_token(t)) == 2:\n",
    "            targets.append(map_target[t])\n",
    "            contexts.append(torch.tensor(c))\n",
    "            \n",
    "    # contexts of shape (N_dataset, context_size)\n",
    "    # targets of shape  (N_dataset)\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    # Create a pytorch dataset out of these context / target pairs\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    # If already generated\n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        # Create context / target dataset based on the list of strings\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train_embedding = load_dataset(words_train, vocab, \"data_train_embedding.pt\")\n",
    "data_val_embedding = load_dataset(words_val, vocab, \"data_val_embedding.pt\")\n",
    "data_test_embedding = load_dataset(words_test, vocab, \"data_test_embedding.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyEmbedding1(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
    "\n",
    "        # Regular MLP         \n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size*2, 256)\n",
    "        self.fc2 = nn.Linear(256, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        \n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class MyEmbedding2(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
    "\n",
    "        # Regular MLP         \n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size*2, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))\n",
    "        out = F.relu(self.fc2(torch.flatten(out, 1)))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "class MyEmbedding3(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
    "\n",
    "        # Regular MLP         \n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size*2, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))\n",
    "        out = F.relu(self.fc2(torch.flatten(out, 1)))\n",
    "        out = F.relu(self.fc3(torch.flatten(out, 1)))\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Task 2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cpu.\n",
      "On device cpu.\n",
      "21:09:03.683469  |  Epoch 1  |  Training loss 4.38984 | Validation loss 3.85965\n",
      "21:10:09.728434  |  Epoch 2  |  Training loss 3.75672 | Validation loss 3.63937\n",
      "21:11:27.971160  |  Epoch 3  |  Training loss 3.57509 | Validation loss 3.54134\n",
      "21:12:59.919001  |  Epoch 4  |  Training loss 3.48000 | Validation loss 3.49218\n",
      "21:14:24.115895  |  Epoch 5  |  Training loss 3.41934 | Validation loss 3.45906\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABf7klEQVR4nO3dd3hUdd7+8fdMeieBNCAECCGhiyIaUGmhCIvi6uoqj+AjuzZQWHVXWbs+CgoWEAXEws+CWLGiSAcBBYFgFAg1ECAQaippM+f3x0AglCEJSc5Mcr+uay4zp8x8DkOY2+/5FothGAYiIiIidYTV7AJEREREqpPCjYiIiNQpCjciIiJSpyjciIiISJ2icCMiIiJ1isKNiIiI1CkKNyIiIlKneJpdQG2z2+3s27ePoKAgLBaL2eWIiIhIBRiGQW5uLo0bN8Zqdd42U+/Czb59+4iJiTG7DBEREamCjIwMmjZt6vSYehdugoKCAMcfTnBwsMnViIiISEXk5OQQExNT9j3uTL0LNydvRQUHByvciIiIuJmKdClRh2IRERGpUxRuREREpE5RuBEREZE6pd71uRERkepls9koKSkxuwypA7y9vS84zLsiFG5ERKRKDMNg//79HDt2zOxSpI6wWq20aNECb2/vi3odhRsREamSk8EmIiICf39/TYwqF+XkJLuZmZk0a9bsov4+KdyIiEil2Wy2smDTsGFDs8uROiI8PJx9+/ZRWlqKl5dXlV/HZToUjx8/HovFwpgxYyp0/OzZs7FYLAwZMqRG6xIRkbOd7GPj7+9vciVSl5y8HWWz2S7qdVwi3KxZs4bp06fTsWPHCh2fnp7Oww8/zNVXX13DlYmIiDO6FSXVqbr+PpkebvLy8hg6dCgzZswgNDT0gsfbbDaGDh3KM888Q8uWLS94fFFRETk5OeUeIiIiUneZHm5GjhzJoEGDSE5OrtDxzz77LBEREYwYMaJCx48bN46QkJCyhxbNFBERqdtMDTezZ89m3bp1jBs3rkLH//zzz7zzzjvMmDGjwu8xduxYsrOzyx4ZGRlVLVdEROScmjdvzmuvvVbh45csWYLFYqnxYfQzZ86kQYMGNfoersi00VIZGRmMHj2a+fPn4+vre8Hjc3Nzuf3225kxYwaNGjWq8Pv4+Pjg4+NzMaVW2L5jx8ktLCUh6sIrloqISO27UJ+Op556iqeffrrSr7tmzRoCAgIqfHy3bt3IzMwkJCSk0u8lF2ZauFm7di1ZWVlceumlZdtsNhvLli1jypQpFBUV4eHhUbZv+/btpKenM3jw4LJtdrsdAE9PT9LS0oiLi6u9CzjDD6mZjJ6dQrsmwXx5bzd1shMRcUGZmZllP3/yySc8+eSTpKWllW0LDAws+9kwDGw2G56eF/6qDA8Pr1Qd3t7eREVFVeocqTjTbkv16dOH1NRUUlJSyh5dunRh6NChpKSklAs2AImJiWcdf91119GrVy9SUlJM70tzWfNQrFZYv/sYS9IOmlqLiIgZDMOgoLjUlIdhGBWqMSoqquwREhKCxWIpe75582aCgoL44YcfuOyyy/Dx8eHnn39m+/btXH/99URGRhIYGMjll1/OggULyr3umbelLBYLb7/9NjfccAP+/v7Ex8fzzTfflO0/87bUydtH8+bNo02bNgQGBjJgwIByYay0tJQHHniABg0a0LBhQx555BGGDx9e6SlRpk6dSlxcHN7e3iQkJPDBBx+U+wyffvppmjVrho+PD40bN+aBBx4o2//mm28SHx+Pr68vkZGR3HTTTZV679piWstNUFAQ7du3L7ctICCAhg0blm0fNmwYTZo0Ydy4cfj6+p51/Mn7iGduN0NEkC/Dk5ozfdkOJv6URs+EcLXeiEi9crzERtsn55ny3huf7Y+/d/V8pT366KNMnDiRli1bEhoaSkZGBgMHDuT555/Hx8eH999/n8GDB5OWlkazZs3O+zrPPPMML730EhMmTOD1119n6NCh7Nq1i7CwsHMeX1BQwMSJE/nggw+wWq38z//8Dw8//DAfffQRAC+++CIfffQR7733Hm3atGHSpEl89dVX9OrVq8LXNmfOHEaPHs1rr71GcnIy3333Hf/7v/9L06ZN6dWrF1988QWvvvoqs2fPpl27duzfv58NGzYA8Ntvv/HAAw/wwQcf0K1bN44cOcLy5csr8Sdbe1x6huLdu3dXywJateXuHnF8+Msu/tyXw7w/9zOgfbTZJYmISCU9++yz9O3bt+x5WFgYnTp1Knv+3HPPMWfOHL755htGjRp13te54447uPXWWwF44YUXmDx5MqtXr2bAgAHnPL6kpIRp06aVdbEYNWoUzz77bNn+119/nbFjx3LDDTcAMGXKFObOnVupa5s4cSJ33HEH9913HwAPPvggv/zyCxMnTqRXr17s3r2bqKgokpOT8fLyolmzZnTt2hVwfCcHBATwl7/8haCgIGJjY+ncuXOl3r+2uFS4WbJkidPnZ5o5c2aN1VIVYQHejLiqBZMXbeOV+Vvo2zYKD6tab0SkfvDz8mDjs/1Ne+/q0qVLl3LP8/LyePrpp/n+++/JzMyktLSU48ePs3v3bqevc/rEtAEBAQQHB5OVlXXe4/39/cv1HY2Oji47Pjs7mwMHDpQFDQAPDw8uu+yysv6nFbFp0ybuuuuuctu6d+/OpEmTAPjb3/7Ga6+9RsuWLRkwYAADBw5k8ODBeHp60rdvX2JjY8v2DRgwoOy2m6txn2YRNzHi6pYE+3qy5UAe3/2+z+xyRERqjcViwd/b05RHdXYDOHPU08MPP8ycOXN44YUXWL58OSkpKXTo0IHi4mKnr3Pm2kgWi8VpEDnX8RXtS1RdYmJiSEtL480338TPz4/77ruPa665hpKSEoKCgli3bh0ff/wx0dHRPPnkk3Tq1MklV4VXuKlmIX5e3HWNY+bk1xZspdRW8UQtIiKuZ8WKFdxxxx3ccMMNdOjQgaioKNLT02u1hpCQECIjI1mzZk3ZNpvNxrp16yr1Om3atGHFihXltq1YsYK2bduWPffz82Pw4MFMnjyZJUuWsGrVKlJTUwHH6OTk5GReeuklfv/9d9LT01m0aNFFXFnNcKnbUnXFHd1b8O6KdHYeymfO+r38rYtmRRYRcVfx8fF8+eWXDB48GIvFwhNPPFGpW0HV5f7772fcuHG0atWKxMREXn/9dY4ePVqpVqt///vf3HzzzXTu3Jnk5GS+/fZbvvzyy7LRXzNnzsRms3HFFVfg7+/Phx9+iJ+fH7GxsXz33Xfs2LGDa665htDQUObOnYvdbichIaGmLrnK1HJTAwJ9PLmnh6P1ZtLCrRSXqvVGRMRdvfLKK4SGhtKtWzcGDx5M//79y83RVlseeeQRbr31VoYNG0ZSUhKBgYH079+/QhPhnjRkyBAmTZrExIkTadeuHdOnT+e9996jZ8+egGMU8owZM+jevTsdO3ZkwYIFfPvttzRs2JAGDRrw5Zdf0rt3b9q0acO0adP4+OOPadeuXQ1dcdVZjNq+oWeynJwcQkJCyM7OJjg4uMbe53ixjWsmLOZgbhH/N6Q9/3NlbI29l4hIbSssLGTnzp20aNGiUl+uUn3sdjtt2rTh5ptv5rnnnjO7nGrh7O9VZb6/1XJTQ/y8PRjZ09HrfcqibRSW2EyuSERE3NmuXbuYMWMGW7ZsITU1lXvvvZedO3dy2223mV2ay1G4qUG3XtGMxiG+7M8pZNavzocMioiIOGO1Wpk5cyaXX3453bt3JzU1lQULFtCmTRuzS3M56lBcg3w8Pbi/Tzxjv0zlzSXb+HvXmGqbQVNEROqXmJiYs0Y6ybmp5aaG3XRZU5qF+XMor5j3V+0yuxwREZE6T+Gmhnl5WBndJx6AaUu3k1tYYnJFIiIidZvCTS0Y0rkJceEBHCso4d2f080uR0REpE5TuKkFHlYLY5JbA/D28h0cK3A+ZbeIiIhUncJNLRnUIZrEqCByi0qZsXyH2eWIiIjUWQo3tcRqtfBgX0frzXsr0jmcV2RyRSIiUlU9e/ZkzJgxZc+bN2/Oa6+95vQci8XCV199ddHvXV2v48zTTz/NJZdcUqPvUZMUbmpR37aRdGwaQkGxjWlLt5tdjohIvTN48GAGDBhwzn3Lly/HYrHw+++/V/p116xZw1133XWx5ZVzvoCRmZnJtddeW63vVdco3NQii+VU6837q3ZxIKfQ5IpEROqXESNGMH/+fPbs2XPWvvfee48uXbrQsWPHSr9ueHg4/v7+1VHiBUVFReHj41Mr7+WuFG5qWY/W4XSJDaWo1M4bi7eZXY6ISL3yl7/8hfDwcGbOnFlue15eHp999hkjRozg8OHD3HrrrTRp0gR/f386dOjAxx9/7PR1z7wttXXrVq655hp8fX1p27Yt8+fPP+ucRx55hNatW+Pv70/Lli154oknKClxTBcyc+ZMnnnmGTZs2IDFYsFisZTVfOZtqdTUVHr37o2fnx8NGzbkrrvuIi8vr2z/HXfcwZAhQ5g4cSLR0dE0bNiQkSNHlr1XRdjtdp599lmaNm2Kj48Pl1xyCT/++GPZ/uLiYkaNGkV0dDS+vr7ExsYybtw4AAzD4Omnn6ZZs2b4+PjQuHFjHnjggQq/d1VoutxaZrFYeLBfa26b8Ssfr97NXde0pGlo7aR9EZEaZRhQUmDOe3v5g8VywcM8PT0ZNmwYM2fO5LHHHsNy4pzPPvsMm83GrbfeSl5eHpdddhmPPPIIwcHBfP/999x+++3ExcXRtWvXC76H3W7nr3/9K5GRkfz6669kZ2eX659zUlBQEDNnzqRx48akpqbyz3/+k6CgIP7zn/9wyy238Mcff/Djjz+yYMECAEJCQs56jfz8fPr3709SUhJr1qwhKyuLf/zjH4waNapcgFu8eDHR0dEsXryYbdu2ccstt3DJJZfwz3/+84LXAzBp0iRefvllpk+fTufOnXn33Xe57rrr+PPPP4mPj2fy5Ml88803fPrppzRr1oyMjAwyMjIA+OKLL3j11VeZPXs27dq1Y//+/WzYsKFC71tVCjcm6BbXiG5xDVm5/TBTFm1j/I2VbwIVEXE5JQXwQmNz3vu/+8A7oEKH3nnnnUyYMIGlS5fSs2dPwHFL6sYbbyQkJISQkBAefvjhsuPvv/9+5s2bx6efflqhcLNgwQI2b97MvHnzaNzY8efxwgsvnNVP5vHHHy/7uXnz5jz88MPMnj2b//znP/j5+REYGIinpydRUVHnfa9Zs2ZRWFjI+++/T0CA4/qnTJnC4MGDefHFF4mMjAQgNDSUKVOm4OHhQWJiIoMGDWLhwoUVDjcTJ07kkUce4e9//zsAL774IosXL+a1117jjTfeYPfu3cTHx3PVVVdhsViIjY0tO3f37t1ERUWRnJyMl5cXzZo1q9Cf48XQbSmTPNTP0ffms7V7SD+Ub3I1IiL1R2JiIt26dePdd98FYNu2bSxfvpwRI0YAYLPZeO655+jQoQNhYWEEBgYyb948du+u2ALImzZtIiYmpizYACQlJZ113CeffEL37t2JiooiMDCQxx9/vMLvcfp7derUqSzYAHTv3h273U5aWlrZtnbt2uHh4VH2PDo6mqysrAq9R05ODvv27aN79+7ltnfv3p1NmzYBjltfKSkpJCQk8MADD/DTTz+VHfe3v/2N48eP07JlS/75z38yZ84cSktLK3WdlaWWG5NcFhtGz4RwlqQdZPLCrbxyyyVmlyQicnG8/B0tKGa9dyWMGDGC+++/nzfeeIP33nuPuLg4evToAcCECROYNGkSr732Gh06dCAgIIAxY8ZQXFx9E7CuWrWKoUOH8swzz9C/f39CQkKYPXs2L7/8crW9x+m8vLzKPbdYLNjt9mp7/UsvvZSdO3fyww8/sGDBAm6++WaSk5P5/PPPiYmJIS0tjQULFjB//nzuu+++spazM+uqLmq5MdFDfRMAmJOyl21ZuSZXIyJykSwWx60hMx4V6G9zuptvvhmr1cqsWbN4//33ufPOO8v636xYsYLrr7+e//mf/6FTp060bNmSLVu2VPi127RpQ0ZGBpmZmWXbfvnll3LHrFy5ktjYWB577DG6dOlCfHw8u3aVX1zZ29sbm812wffasGED+fmn7gCsWLECq9VKQkJChWt2Jjg4mMaNG5+1IvmKFSto27ZtueNuueUWZsyYwSeffMIXX3zBkSNHAPDz82Pw4MFMnjyZJUuWsGrVKlJTU6ulvnNRuDFRh6Yh9G8XiWHAqwu2ml2OiEi9ERgYyC233MLYsWPJzMzkjjvuKNsXHx/P/PnzWblyJZs2beLuu+/mwIEDFX7t5ORkWrduzfDhw9mwYQPLly/nscceK3dMfHw8u3fvZvbs2Wzfvp3JkyczZ86ccsc0b96cnTt3kpKSwqFDhygqOnvy16FDh+Lr68vw4cP5448/WLx4Mffffz+33357WX+b6vDvf/+bF198kU8++YS0tDQeffRRUlJSGD16NACvvPIKH3/8MZs3b2bLli189tlnREVF0aBBA2bOnMk777zDH3/8wY4dO/jwww/x8/Mr1y+nuincmOxffVtjscD3v2eycV+O2eWIiNQbI0aM4OjRo/Tv379c/5jHH3+cSy+9lP79+9OzZ0+ioqIYMmRIhV/XarUyZ84cjh8/TteuXfnHP/7B888/X+6Y6667jn/961+MGjWKSy65hJUrV/LEE0+UO+bGG29kwIAB9OrVi/Dw8HMOR/f392fevHkcOXKEyy+/nJtuuok+ffowZcqUyv1hXMADDzzAgw8+yEMPPUSHDh348ccf+eabb4iPjwccI79eeuklunTpwuWXX056ejpz587FarXSoEEDZsyYQffu3enYsSMLFizg22+/pWHDhtVa4+kshmEYNfbqLignJ4eQkBCys7MJDg42uxwA7v94Pd9u2Edym0jeHt7F7HJERC6osLCQnTt30qJFC3x9fc0uR+oIZ3+vKvP9rZYbFzAmOR6rBRZsOsCGjGNmlyMiIuLWFG5cQFx4IDd0bgrAy/Mr3mlNREREzqZw4yJG94nH02ph2ZaDrEk/YnY5IiIibkvhxkU0a+jPzZfHADBxXhr1rCuUiIhItVG4cSGjerXC28PKrzuPsHL7YbPLERG5IP2PmFSn6vr7pHDjQho38OO2K5oBMPEntd6IiOs6ObNsQYFJC2VKnXRyFujTl4qoCi2/4GLu6xXH7DW7Wb/7GEvSDtIrMcLskkREzuLh4UGDBg3K1ify9/cvm+FXpCrsdjsHDx7E398fT8+LiycKNy4mIsiX4UnNmb5sBxN/SqNnQrj+wRARl3RyteqKLsAociFWq5VmzZpd9Peewo0LurtHHB/+sos/9+Uw78/9DGgfbXZJIiJnsVgsREdHExERQUlJidnlSB3g7e2N1XrxPWYUblxQWIA3d17VgtcXbeOV+Vvo2zYKD6tab0TENXl4eFx0HwmR6qQOxS7qH1e3JNjXky0H8vju931mlyMiIuI2FG5cVIifF3dd0xKA1xZspdRmN7kiERER96Bw48Lu6N6CUH8vdh7KZ876vWaXIyIi4hYUblxYoI8n9/aMA2DSwq0Ul6r1RkRE5EIUblzc7Vc2JzzIhz1Hj/PpbxlmlyMiIuLyFG5cnJ+3ByNPtN5MWbSNwhKbyRWJiIi4NoUbN3DrFc1oHOLL/pxCZv262+xyREREXJrCjRvw8fTg/j7xALy5ZBsFxaUmVyQiIuK6FG7cxE2XNaVZmD+H8op5f9Uus8sRERFxWQo3bsLLw8roE60305ZuJ7dQU52LiIici8KNGxnSuQlx4QEcKyjh3Z/TzS5HRETEJSncuBEPq4Uxya0BeHv5Do4VFJtckYiIiOtRuHEzgzpEkxgVRG5RKTOW7zC7HBEREZejcONmrFYLD/Z1tN68tyKdw3lFJlckIiLiWhRu3FDftpF0bBpCQbGNaUu3m12OiIiIS1G4cUMWy6nWm/dX7eJATqHJFYmIiLgOhRs31aN1OF1iQykqtfPm4m1mlyMiIuIyFG7clMVi4cF+jtabWat3s+dogckViYiIuAaFGzfWLa4R3eIaUmIzmLJIrTciIiKgcOP2HjrRevPZ2j2kH8o3uRoRERHzKdy4uctiw+iZEI7NbjB54VazyxERETGdwk0d8FDfBADmpOxlW1auydWIiIiYS+GmDujQNIT+7SIxDHh1gVpvRESkfnOZcDN+/HgsFgtjxow57zEzZszg6quvJjQ0lNDQUJKTk1m9enXtFenC/tW3NRYLfP97Jhv35ZhdjoiIiGlcItysWbOG6dOn07FjR6fHLVmyhFtvvZXFixezatUqYmJi6NevH3v37q2lSl1XYlQwf+nYGIBX5m8xuRoRERHzmB5u8vLyGDp0KDNmzCA0NNTpsR999BH33Xcfl1xyCYmJibz99tvY7XYWLlxYS9W6tjHJ8VgtsGDTATZkHDO7HBEREVOYHm5GjhzJoEGDSE5OrvS5BQUFlJSUEBYWdt5jioqKyMnJKfeoq+LCA7mhc1MAXlbrjYiI1FOmhpvZs2ezbt06xo0bV6XzH3nkERo3buw0GI0bN46QkJCyR0xMTFXLdQuj+8TjabWwbMtB1qQfMbscERGRWmdauMnIyGD06NF89NFH+Pr6Vvr88ePHM3v2bObMmeP0/LFjx5KdnV32yMjIuJiyXV6zhv78rYsjwE2cl4ZhGCZXJCIiUrtMCzdr164lKyuLSy+9FE9PTzw9PVm6dCmTJ0/G09MTm8123nMnTpzI+PHj+emnny7YCdnHx4fg4OByj7ru/t6t8Paw8uvOI6zcftjsckRERGqVaeGmT58+pKamkpKSUvbo0qULQ4cOJSUlBQ8Pj3Oe99JLL/Hcc8/x448/0qVLl1qu2j00buDHbVc0A2DiT2q9ERGR+sXTrDcOCgqiffv25bYFBATQsGHDsu3Dhg2jSZMmZX1yXnzxRZ588klmzZpF8+bN2b9/PwCBgYEEBgbW7gW4uPt6xTF7zW7W7z7GkrSD9EqMMLskERGRWmH6aClndu/eTWZmZtnzqVOnUlxczE033UR0dHTZY+LEiSZW6ZoignwZntQcUOuNiIjULxajnn3r5eTkEBISQnZ2dp3vf3Mkv5irX1xEfrGNaf9zKQPaR5tdkoiISJVU5vvbpVtu5OKEBXhz51UtAMesxTZ7vcqxIiJSTync1HH/uLolwb6ebDmQx3e/7zO7HBERkRqncFPHhfh5cdc1LQF4bcFWSm12kysSERGpWQo39cAd3VsQ6u/FzkP5zFmvRUZFRKRuU7ipBwJ9PLm3ZxwAkxZupbhUrTciIlJ3KdzUE7df2ZzwIB/2HD3OZ2vr9hIUIiJSvync1BN+3h6MPNF68/rCbRSWnH95CxEREXemcFOP3HpFMxqH+LI/p5BZv+42uxwREZEaoXBTj/h4enB/n3gA3lyynYLiUpMrEhERqX4KN/XMTZc1pVmYP4fyinh/1S6zyxEREal2Cjf1jJeHldEnWm+mLd1ObmGJyRWJiIhUL4WbemhI5ybEhQdwrKCE91akm12OiIhItVK4qYc8rBbGJLcGYMayHRwrKDa5IhERkeqjcFNPDeoQTWJUELlFpcxYvsPsckRERKqNwk09ZbVaeLCvo/XmvRXpHM4rMrkiERGR6qFwU4/1bRtJx6YhFBTbmLZ0u9nliIiIVAuFm3rMYjnVevP+ql0cyCk0uSIREZGLp3BTz/VoHU6X2FCKSu28uXib2eWIiIhcNIWbes5isfBgP0frzazVu9lztMDkikRERC6Owo3QLa4R3eIaUmIzmLJIrTciIuLeFG4EgIdOtN58tnYP6YfyTa5GRESk6hRuBIDLYsPomRCOzW4weeFWs8sRERGpMoUbKfNQ3wQA5qTsZVtWrsnViIiIVI3CjZTp0DSEfm0jMQx4dYFab0RExD0p3Eg5D/ZrjcUC3/+eycZ9OWaXIyIiUmkKN1JOYlQwf+nYGIBX5m8xuRoREZHKU7iRs4xJjsdqgQWbDrAh45jZ5YiIiFSKwo2cJS48kBs6NwXgZbXeiIiIm1G4kXMa3SceT6uFZVsOsib9iNnliIiIVJjCjZxTs4b+/K1LDAAT56VhGIbJFYmIiFSMwo2c1/29W+HtYeXXnUdYuf2w2eWIiIhUiMKNnFfjBn7cdkUzACb+pNYbERFxDwo34tR9veLw9bKyfvcxlqQdNLscERGRC1K4EacignwZntQcgJfnq/VGRERcn8KNXNDdPeII8Pbgj705zPvzgNnliIiIOKVwIxcUFuDNnVe1AOCV+WnY7Gq9ERER16VwIxXyj6tbEuzryZYDeXz3+z6zyxERETkvhRupkBA/L+66piUAkxZspdRmN7kiERGRc1O4kQq7o3sLQv292HEonznr95pdjoiIyDkp3EiFBfp4cm/POAAmLdxKcalab0RExPUo3Eil3H5lc8KDfNhz9Difrc0wuxwREZGzKNxIpfh5ezDyROvN6wu3UVhiM7kiERGR8hRupNJuvaIZjUN82Z9TyKxfd5tdjoiISDkKN1JpPp4e3N8nHoA3l2ynoLjU5IpEREROUbiRKrnpsqY0C/PnUF4R76/aZXY5IiIiZRRupEq8PKyMPtF6M23pdnILS0yuSERExEHhRqpsSOcmxIUHcKyghPdWpJtdjoiICKBwIxfBw2phTHJrAGYs28GxgmKTKxIREVG4kYs0qEM0iVFB5BaVMmP5DrPLERERUbiRi2O1WvhXX0frzXsr0jmcV2RyRSIiUt8p3MhF69c2kg5NQigotjFt6XazyxERkXpO4UYumsVi4aF+jtab91ft4kBOockViYhIfaZwI9WiR+twLosNpajUzpuLt5ldjoiI1GMKN1ItTm+9mbV6N3uOFphckYiI1FcKN1JtusU1oltcQ0psBlMWqfVGRETMoXAj1epk681na/eQfijf5GpERKQ+UriRanVZbBg9E8Kx2Q0mL9xqdjkiIlIPKdxItXuobwIAc1L2si0r1+RqRESkvnGZcDN+/HgsFgtjxoxxetxnn31GYmIivr6+dOjQgblz59ZOgVJhHZqG0K9tJIYBry5Q642IiNQulwg3a9asYfr06XTs2NHpcStXruTWW29lxIgRrF+/niFDhjBkyBD++OOPWqpUKurBfq2xWOD73zPZuC/H7HJERKQeMT3c5OXlMXToUGbMmEFoaKjTYydNmsSAAQP497//TZs2bXjuuee49NJLmTJlSi1VKxWVGBXMXzo2BuCV+VtMrkZEROoT08PNyJEjGTRoEMnJyRc8dtWqVWcd179/f1atWnXec4qKisjJySn3kNoxJjkeqwUWbDrAhoxjZpcjIiL1hKnhZvbs2axbt45x48ZV6Pj9+/cTGRlZbltkZCT79+8/7znjxo0jJCSk7BETE3NRNUvFxYUHckPnpgC8rNYbERGpJaaFm4yMDEaPHs1HH32Er69vjb3P2LFjyc7OLntkZGTU2HvJ2Ub3icfTamHZloOsST9idjkiIlIPmBZu1q5dS1ZWFpdeeimenp54enqydOlSJk+ejKenJzab7axzoqKiOHDgQLltBw4cICoq6rzv4+PjQ3BwcLmH1J5mDf35WxdHa9nEeWkYhmFyRSIiUteZFm769OlDamoqKSkpZY8uXbowdOhQUlJS8PDwOOucpKQkFi5cWG7b/PnzSUpKqq2ypQru790Kbw8rv+48wsrth80uR0RE6jhPs944KCiI9u3bl9sWEBBAw4YNy7YPGzaMJk2alPXJGT16ND169ODll19m0KBBzJ49m99++4233nqr1uuXimvcwI/brmjGzJXpvPxTGt3iGmKxWMwuS0RE6ijTR0s5s3v3bjIzM8ued+vWjVmzZvHWW2/RqVMnPv/8c7766quzQpK4nvt6xeHrZWXd7mMsSTtodjkiIlKHWYx61gkiJyeHkJAQsrOz1f+mlo2bu4npy3bQvkkw3466Sq03IiJSYZX5/nbplhupW+7uEUeAtwd/7M1h3p8HLnyCiIhIFSjcSK0JC/DmzqtaAPDK/DRs9nrVaCgiIrVE4UZq1T+ubkmwrydbDuTx3e/7zC5HRETqIIUbqVUhfl7cdU1LACYt2EqpzW5yRSIiUtco3Eitu6N7C0L9vdhxKJ856/eaXY6IiNQxCjdS6wJ9PLm3ZxwAkxZupbhUrTciIlJ9FG7EFLdf2ZzwIB/2HD3OZ2u13peIiFQfhRsxhZ+3ByNPtN68vnAbhSVnryUmIiJSFQo3Yppbr2hG4xBf9ucUMuvX3WaXIyIidYTCjZjGx9ODUb3jAXhzyXYKiktNrkhEROoChRsx1d+6NKVZmD+H8op4f9Uus8sREZE6QOFGTOXlYWV0H0frzbSl28ktLDG5IhERcXcKN2K6IZ2b0DI8gGMFJby3It3sckRExM0p3IjpPKwW/pXcGoAZy3ZwrKDY5IpERMSdKdyISxjUIZrEqCByi0qZsXyH2eWIiIgbq1K4ycjIYM+ePWXPV69ezZgxY3jrrbeqrTCpX6xWC//q62i9eW9FOofzikyuSERE3FWVws1tt93G4sWLAdi/fz99+/Zl9erVPPbYYzz77LPVWqDUH/3aRtKhSQgFxTamLd1udjkiIuKmqhRu/vjjD7p27QrAp59+Svv27Vm5ciUfffQRM2fOrM76pB6xWCw81M/RevP+ql0cyCk0uSIREXFHVQo3JSUl+Pj4ALBgwQKuu+46ABITE8nMzKy+6qTe6dE6nMtiQykqtfPm4m1mlyMiIm6oSuGmXbt2TJs2jeXLlzN//nwGDBgAwL59+2jYsGG1Fij1y+mtN7NW72bP0QKTKxIREXdTpXDz4osvMn36dHr27Mmtt95Kp06dAPjmm2/KbleJVFW3uEZ0i2tIic1gyiK13oiISOVYDMMwqnKizWYjJyeH0NDQsm3p6en4+/sTERFRbQVWt5ycHEJCQsjOziY4ONjscuQ81u46wo1TV+FhtbDwwR40bxRgdkkiImKiynx/V6nl5vjx4xQVFZUFm127dvHaa6+Rlpbm0sFG3MdlsWH0TAjHZjeYvHCr2eWIiIgbqVK4uf7663n//fcBOHbsGFdccQUvv/wyQ4YMYerUqdVaoNRfD/VNAOCrlL1sy8o1uRoREXEXVQo369at4+qrrwbg888/JzIykl27dvH+++8zefLkai1Q6q8OTUPo1zYSuwGvLlDrjYiIVEyVwk1BQQFBQUEA/PTTT/z1r3/FarVy5ZVXsmvXrmotUOq3B/u1xmKB73/PZOO+HLPLERERN1ClcNOqVSu++uorMjIymDdvHv369QMgKytLnXSlWiVGBfOXjo0BeHXBFpOrERERd1ClcPPkk0/y8MMP07x5c7p27UpSUhLgaMXp3LlztRYoMiY5HqsF5m88wIaMY2aXIyIiLq5K4eamm25i9+7d/Pbbb8ybN69se58+fXj11VerrTgRgLjwQG7o3BSAl+er9UZERJyrUrgBiIqKonPnzuzbt69shfCuXbuSmJhYbcWJnDS6TzyeVgvLthxkTfoRs8sREREXVqVwY7fbefbZZwkJCSE2NpbY2FgaNGjAc889h91ur+4aRWjW0J+/dYkBYOK8NKo496SIiNQDVQo3jz32GFOmTGH8+PGsX7+e9evX88ILL/D666/zxBNPVHeNIgDc37sV3h5Wft15hJXbD5tdjoiIuKgqLb/QuHFjpk2bVrYa+Elff/019913H3v37q22Aqubll9wb09/8yczV6ZzabMGfHFvNywWi9kliYhILajx5ReOHDlyzr41iYmJHDmi/hBSc+7rFYevl5V1u4+xJO2g2eWIiIgLqlK46dSpE1OmTDlr+5QpU+jYseNFFyVyPhFBvgxPag7Ay/PV90ZERM7mWZWTXnrpJQYNGsSCBQvK5rhZtWoVGRkZzJ07t1oLFDnT3T3i+PCXXfyxN4d5fx5gQPsos0sSEREXUqWWmx49erBlyxZuuOEGjh07xrFjx/jrX//Kn3/+yQcffFDdNYqUExbgzZ1XtQDglflp2OxqvRERkVOq1KH4fDZs2MCll16KzWarrpesdupQXDdkHy/h6hcXkVNYyqS/X8L1lzQxuyQREalBNd6hWMRsIX5e/PPqlgBMWrCVUpvmVxIREQeFG3Fb/3tVC0L9vdhxKJ856113+gEREaldCjfVxW6D2UNh4zdmV1JvBPp4cm/POAAmLdxKcalab0REpJKjpf7617863X/s2LGLqcW9rf8QNn/neHT4G1z7EviHmV1VnXf7lc2ZsXwne44e57O1GQy9ItbskkRExGSVarkJCQlx+oiNjWXYsGE1Vatr6/R3uPohsFgh9TN4Mwm2/GR2VXWen7cHI0+03ry+cBuFJa7bmV1ERGpHtY6Wcgc1Plpqz1r46h44tMXxvPP/QP8XwDek+t9LACgqtdFrwhL2ZRfy5F/alg0TFxGRukOjpczU9DK4exkkjQIsjttVb3aD7YvNrqzO8vH0YFTveADeXLKdguJSkysSEREzKdzUBC8/6P88/O9cCG0BOXvggyHw3YNQlGd2dXXS37o0pVmYP4fyinh/1S6zyxERERMp3NSk2G5w7wq4/J+O57+9A9O6w66V5tZVB3l5WBndx9F6M23pdnILS0yuSEREzKJwU9O8A2DQRBj2NYTEwNF0eG8g/PhfKDludnV1ypDOTWgZHsCxghLeW5FudjkiImIShZva0rIn3LsSOt8OGPDLGzDtatjzm9mV1RkeVgv/Sm4NwIxlOzhWUGxyRSIiYgaFm9rkGwzXT4HbPoPAKDi8Fd7pCwuegdIis6urEwZ1iCYxKojcolJmLN9hdjkiImIChRsztO4H962CjreAYYefX4G3ekHmBrMrc3tWq4V/9XW03ry3Ip3DeQqNIiL1jcKNWfzD4K9vwS0fgn8jyPoTZvSGJePBps6wF6Nf20g6NAmhoNjGtKXbzS5HRERqmcKN2doMhpG/QpvrwF4KS8bB233gwEazK3NbFouFh/o5Wm/eX7WLrJxCkysSEZHapHDjCgIawc3vw43vgF+o4/bUWz3g51cdC3JKpfVoHc5lsaEUldp5Y/E2s8sREZFapHDjKiwW6HAT3PcLtB4AtmJY8DS82x8O6cu5sk5vvfl4dQZ7j2nYvYhIfaFw42qCouDW2XD9m+ATDHvWOCb+W/Um2O1mV+dWusU1oltcQ4ptdqYs2mp2OSIiUksUblyRxQKdhzpGVLXsBaWFMG8s/L+/wJGdZlfnVk623nz62x7SD+WbXI2IiNQGhRtXFtIUbp8Df3kVvAJg1wqY2h3WvAP1azH3KrssNoyeCeHY7AaTF6r1RkSkPlC4cXUWC3S507FGVexVUJIP3z8IH9wA2XvMrs4tPNQ3AYCvUvayLSvX5GpERKSmmRpupk6dSseOHQkODiY4OJikpCR++OEHp+e89tprJCQk4OfnR0xMDP/6178oLKwHQ33DWsDwb2HAePD0hR2L4c0kWP+RWnEuoEPTEPq1jcRuwKsL1HojIlLXmRpumjZtyvjx41m7di2//fYbvXv35vrrr+fPP/885/GzZs3i0Ucf5amnnmLTpk288847fPLJJ/z3v/+t5cpNYrXClffCPSug6eVQlANf3wcf/x1y95tdnUt7sF9rLBb4/vdMNu7LMbscERGpQRbDcK3/7Q8LC2PChAmMGDHirH2jRo1i06ZNLFy4sGzbQw89xK+//srPP/98ztcrKiqiqOjUFPw5OTnExMSQnZ1NcHBw9V9AbbHbYOXrsPh5x7Bx3wYw6GVof6PjVpac5f6P1/Pthn30bRvJjGFdzC5HREQqIScnh5CQkAp9f7tMnxubzcbs2bPJz88nKSnpnMd069aNtWvXsnr1agB27NjB3LlzGThw4Hlfd9y4cYSEhJQ9YmJiaqT+Wmf1gKvGwF1LIboTFB6DL0bAp8Mg/5DZ1bmkMcnxWC0wf+MBNmQcM7scERGpIaa33KSmppKUlERhYSGBgYHMmjXLaViZPHkyDz/8MIZhUFpayj333MPUqVPPe3ydbbk5na0Elr8Cy15yLOHg38gxwqrtdWZX5nIe+nQDX6zbwzWtw3n/zq5mlyMiIhXkVi03CQkJpKSk8Ouvv3LvvfcyfPhwNm4897pKS5Ys4YUXXuDNN99k3bp1fPnll3z//fc899xz5319Hx+fsg7LJx91jocX9HwE/rkIItpCwSH49Hb44p9w/KjZ1bmU0X3i8bRaWLblIGvSj5hdjoiI1ADTW27OlJycTFxcHNOnTz9r39VXX82VV17JhAkTyrZ9+OGH3HXXXeTl5WG1XjirVSb5uaXSIsfK4iteA8MOgVFw3evQup/ZlbmMsV+m8vHq3VzRIozZd12JRX2URERcnlu13JzJbreXu410uoKCgrMCjIeHBwAultHM4+kDyU/BiPnQMB7y9sOsv8HXI6FQo4QA7u/dCm8PK7/uPMLK7YfNLkdERKqZqeFm7NixLFu2jPT0dFJTUxk7dixLlixh6NChAAwbNoyxY8eWHT948GCmTp3K7Nmz2blzJ/Pnz+eJJ55g8ODBZSFHTmjaBe5ZDkmjAAus/xCmdoMdS8yuzHSNG/hx2xXNAHj5pzQFYxGROsbTzDfPyspi2LBhZGZmEhISQseOHZk3bx59+/YFYPfu3eVaah5//HEsFguPP/44e/fuJTw8nMGDB/P888+bdQmuzcsP+j8PiYPgq3vhaDq8fz1c/g9IfgZ8As2u0DT39Yxj9prdrNt9jCVpB+mVGGF2SSIiUk1crs9NTavzfW7OpygPFjwFa952PA9tDkOmQmw3U8sy07i5m5i+bAftmwTz7air1PdGRMSFuXWfG6khPoGOSf5u/wpCYhytOO8NhB//CyXHza7OFHf3iCPA24M/9uYw788DZpcjIiLVROGmvonrBfeuhM63Awb88gZMuxr2/GZ2ZbUuLMCbO69qAcAr89Ow2etVI6aISJ2lcFMf+QbD9VPgtk8dQ8UPb4V3+sKCZxxDyeuRf1zdkmBfT7YcyOO73/eZXY6IiFQDhZv6rHV/uG8VdLjZMSfOz6/AW70gc4PZldWaED8v/nl1SwAmLdhKqc1uckUiInKxFG7qO/8wuHEG3PKhY9mGrD9hRm/HRIC2ErOrqxX/e1ULQv292HEonznr95pdjoiIXCSFG3FoMxhG/gptrnOsT7VkHLzdB7I2mV1ZjQv08eTennEATFq4leJStd6IiLgzhRs5JaAR3Pw+3PgO+DZw3J6afg38/CrYbWZXV6Nuv7I54UE+7Dl6nCe//oPsgvrRaiUiUhcp3Eh5Fgt0uMnRitN6ANiKYcHT8G5/OLTN7OpqjJ+3Bw/1bQ3A7DUZ9Ji4mHd/3qlWHBERN6RJ/OT8DANSPoIfx0JRDnj6QvLT0PVuqMAipe5oSVoWL8zdxJYDeQA0b+jPo9e2oX+7SE3yJyJiosp8fyvcyIVl74GvR8GOxY7nsVc5hpKHtTC3rhpSarPz6W97eGV+GofyigHo2jyMxwa1oVNMA3OLExGppxRunFC4qSLDgN/ehZ+egJJ88AqAfs9Blzsdt7LqoLyiUqYt2c6M5TsoOnF76vpLGvPv/gk0DfU3uToRkfpF4cYJhZuLdGQnfD0Sdq1wPI/rDde9DiFNza2rBmVmH2fCvDS+XOcYJu7taWXEVS24t2ccwb5eJlcnIlI/KNw4oXBTDex2+HUaLHwGSgvBJxgGjIdLbquzrTgAf+zN5v++38gvO44A0DDAmzHJ8dzatRmeHnWzD5KIiKtQuHFC4aYaHdoKX90Le9Y4nrceAIMnQVCUuXXVIMMwWLApi3FzN7HjUD4AceEB/HdgG3onRqjTsYhIDVG4cULhpprZbbByMix+wTFs3C8UBk6E9jfW6VacEpudWb/u5rUFWzh6Yk6cbnEN+e/ANrRvEmJydSIidY/CjRMKNzXkwEaYczfs/93xvO31MOgVx8SAdVhOYQlvLN7Gez+nU2yzY7HAXzs35d/9E4gK8TW7PBGROkPhxgmFmxpkK4HlL8OyCY4lHPwbweDXHEs71HEZRwp4aV4a325wrCzu62XlrqtbcnePOAJ8PE2uTkTE/SncOKFwUwv2pTj64mRtdDzvcDMMfMlxy6qOW7/7KM9/v4nfdh0FoFGgDw/1a83NXWLwsNbd23QiIjVN4cYJhZtaUlrkWFl8xWtg2CEwyjFkvHU/syurcYZh8OMf+xn/42Z2HS4AICEyiP8OakOP1uEmVyci4p4UbpxQuKlle36DOffA4a2O551vh/4vgG/d/7MvLrXz/qp0Xl+0jezjjk7H17QO578DE0mMqvvXLyJSnRRunFC4MUHJcVj4HPzyJmBASIxj+YaWPc2urFYcKyjm9UXbeH9VOiU2A6sFbu4Sw4P9WhMRpE7HIiIVoXDjhMKNidJXwNf3wdF0x/PL/wHJz4BPoKll1Zb0Q/m8+ONmfvhjPwD+3h7c0yOOf17dEj9vD5OrExFxbQo3TijcmKwoDxY8BWvedjwPbQ5DpkJsN1PLqk2/pR/h/77fRErGMQCign15uH8Cf+3cBKs6HYuInJPCjRMKNy5i+2LHSuM5ewALJI2E3o+Dl5/ZldUKwzD49vdMXvxhM3uPHQegbXQwjw9qQ7dWdXtuIBGRqlC4cULhxoUUZsO8/8L6Dx3PG7V2tOI07WJuXbWosMTGzJXpvLFoG7lFpQD0SYxg7MA2tIqoH7frREQqQuHGCYUbF7RlHnxzP+QdAIsVuo+Bno+Cp4/ZldWaw3lFTFq4lY9+3Y3NbuBhtXBb12aMSY6nYWD9+XMQETkfhRsnFG5cVMER+OE/kPqZ43lEO7hhKkR3MreuWrb9YB7j5m5mwaYDAAT6eHJfrzju7N4CXy91OhaR+kvhxgmFGxe38Rv47l9QcAisnnDNf+DqB8HDy+zKatXK7Yd4Ye4m/tibA0CTBn78Z0ACgzs2VqdjEamXFG6cULhxA3kH4ft/waZvHc+jL4EbpkFEG1PLqm12u8Gc9XuZMC+N/TmFAHRqGsJjg9rStUWYydWJiNQuhRsnFG7chGFA6ucw92EoPAYe3tDrMeh2P1jr1+2Z48U23vl5B1OXbCe/2AZA/3aRPHptG1o0CjC5OhGR2qFw44TCjZvJyYRvR8PWeY7nTbs6RlQ1amVuXSbIyi3k1flb+WTNbuwGeFot3J4UywO94wkN8Da7PBGRGqVw44TCjRsyDEj5CH4cC0U54OkHyU9B17vBajW7ulq35UAuL8zdxJK0gwAE+3pyf+94hnWLxcezfrVqiUj9oXDjhMKNGzuWAd+Mgh1LHM9jr4IhbzhmOa6Hlm89yPPfb2Lz/lwAmoX588iARAZ2iMJiUadjEalbFG6cULhxc4YBv70DPz0JJfngFQD9noMud0I9/EK32Q0+X5vBxJ+2cDC3CIBLmzXgsUFtuSw21OTqRESqj8KNEwo3dcSRnfD1SNi1wvE8rjdc9zqENDW3LpPkF5Xy1rIdvLVsB8dLHJ2OB3WM5tEBicSE+ZtcnYjIxVO4cULhpg6x2+HXabDwGSgtBJ9gGDAeLrmtXrbiAOzPLuTln9L4fN0eDAO8Pazc0b05I3u1IsSvfs0VJCJ1i8KNEwo3ddChrTDnHtj7m+N562th8GsQFGVqWWb6c182L8zdxIpthwEI9fdidJ94hl4Zi5dH/euELSLuT+HGCYWbOspWCisnw5JxYCsGv1AYOBHa31hvW3EMw2BJ2kGen7uJbVl5ALRoFMCj1ybSr22kOh2LiFtRuHFC4aaOO7AR5twN+393PG97PQx6BQIamVuXiUptdmavyeDV+Vs4nF8MwBUtwnhsUBs6Nm1gbnEiIhWkcOOEwk09YCuBZRNh+USwl4J/I8dtqjaDza7MVLmFJUxdsp13ft5JUakdgBs6N+Hf/RNo3MDP5OpERJxTuHFC4aYe2Zfi6ItzcJPjecdb4NoXHbes6rG9x44zcV4ac9bvBcDH08qIq1pwb884gnzV6VhEXJPCjRMKN/VMaZGjH86KSWDYISjaMWQ8vq/ZlZnu9z3H+L/vN7F65xEAGgV6Mya5NX+/PAZPdToWERejcOOEwk09lbEGvroHDm9zPO98O/R/AXzr998BwzD4aeMBxv+wmZ2H8gFoFRHIfwcm0ishQp2ORcRlKNw4oXBTjxUXwKLn4JepgAEhMXD9FGjZ0+zKTFdis/PRL7uYtHArRwtKAOjeqiGPDWxL28b6PRER8yncOKFwI6SvgK/uhWO7HM8v/yf0fQa8A8ytywVkHy/hjcXbmLkinWKbHYsFbrq0KQ/3TyAy2Nfs8kSkHlO4cULhRgAoyoP5TzrWqQIIbQFDpkJskrl1uYiMIwW8+ONmvvs9EwA/Lw/+eU1L7r6mJQE+niZXJyL1kcKNEwo3Us72RfD1/ZCzB7BA0ki4+iHwDzO7MpewbvdR/u+7jazbfQyAiCAfHurXmpsui8HDqv44IlJ7FG6cULiRsxRmw4//hZQPHc8tVoi5AloPgISB0Ci+3s5yDI5Ox3NT9zP+x01kHDkOQGJUEI8NasPV8eEmVyci9YXCjRMKN3JeaT/Cov+DA6nlt4e1dKxXlTAAmiWBR/2cC6ao1MYHq3YxeeFWcgpLAejROpzHBrWhdWSQydWJSF2ncOOEwo1c0LHdsGUepM2F9J8da1Wd5BsCrZIdYSc+uV5OCHg0v5jJi7bywapdlNoNrBa45fJmPNi3NeFBPmaXJyJ1lMKNEwo3UilFuY5+OWk/wtZ5UHD41D6Lh6MlJ2GAI+w0amVenSbYeSif8T9sYt6fBwAI8Pbg3p5xjLiqJX7eHiZXJyJ1jcKNEwo3UmV2G+z5Dbb84Ag7J5d1OKlhK0i41hF0Yq4Aj/oxqujXHYd5fu4mft+TDUB0iC//7p/AkEuaYFWnYxGpJgo3TijcSLU5stNx+2rLD465c+wlp/b5NoD4fo5WnVbJjttZdZjdbvDt7/t46cc09h5zdDpu3ySYxwa2JSmuocnViUhdoHDjhMKN1IjC7BO3r36ArT/B8aOn9lk9IbbbqU7JYS3Nq7OGFZbYeHfFTt5cvJ28Iken4+Q2kYwdmEhceKDJ1YmIO1O4cULhRmqcrRT2rHYEnS0/wqEt5fc3SjjVTyemK1jrXv+UQ3lFTFqwlVmrd2OzG3hYLQy9ohmj+8TTMFCdjkWk8hRunFC4kVp3eLsj5KT9ALtWgmE7tc8vDFr3d8ypE9e7zi3kuS0rl3FzN7NwcxYAQT6ejOzdiju6NcfXq+6FOhGpOQo3TijciKmOH4NtCxxhZ+tPjttZJ1m9oPlVJzolD4DQWNPKrG4rtx3i/77fxMbMHACaNPDjkWsTGdwxWiuPi0iFKNw4oXAjLsNWChm/nLp9dXhb+f0RbU/MknwtNLnM7W9f2e0GX67fy8R5aezPKQSgU0wDnhjUhi7NtdyFiDjnNuFm6tSpTJ06lfT0dADatWvHk08+ybXXXnvec44dO8Zjjz3Gl19+yZEjR4iNjeW1115j4MCBFXpPhRtxWYe2ngo6u38pf/vKv1H521c+7ts593ixjRnLdzBt6XYKih3XeG37KB69NpHYhlqZXUTOzW3CzbfffouHhwfx8fEYhsH/+3//jwkTJrB+/XratWt31vHFxcV0796diIgI/vvf/9KkSRN27dpFgwYN6NSpU4XeU+FG3ELBEcftq7QfYNtCKDrt9pWHNzS/+tTtqwYx5tV5EbJyCnl1wRY+WZOB3QAvDwu3X9mcB/q0ooG/t9nliYiLcZtwcy5hYWFMmDCBESNGnLVv2rRpTJgwgc2bN+PlVbH1fYqKiigqKip7npOTQ0xMjMKNuA9biaMj8slOyUd3lt8f2eHU6KvGncFqNafOKtq8P4cX5m5m2ZaDAIT4eXF/71YMS2qOt6d7XYuI1By3DDc2m43PPvuM4cOHs379etq2bXvWMQMHDiQsLAx/f3++/vprwsPDue2223jkkUfw8Dh3f4Snn36aZ5555qztCjfilgzDMbT85O2rjF/BsJ/aHxh5YvLAa6FlT/B2n9s8S7cc5IXvN5F2IBeA2Ib+PDogkQHto9TpWETcK9ykpqaSlJREYWEhgYGBzJo167z9ZxITE0lPT2fo0KHcd999bNu2jfvuu48HHniAp5566pznqOVG6rT8w45RV1t+gG2LoDj31D5PX2hxjePWVesBENLEvDorqNRm57O1e3j5py0cynP83naJDeWxQW3o3Kz+LVIqIqe4VbgpLi5m9+7dZGdn8/nnn/P222+zdOnSc7bctG7dmsLCQnbu3FnWUvPKK68wYcIEMjMzK/R+6nMjdVZpMez62bHu1ZYfHKubny6q46l+OtGXuPTtq7yiUt5aup23lu+gsMTRMjW4U2P+0z+BmDB/k6sTETO4Vbg5U3JyMnFxcUyfPv2sfT169MDLy4sFCxaUbfvhhx8YOHAgRUVFeHtfuBOiwo3UC4YBWZtOLfK5Zw1w2q96UPSJ0VfXQsse4OVnWqnOZGYfZ+K8LXy5fg+GAd6eVv63e3NG9mpFsG/F+t2JSN1Qme9vl/tfN7vdXu420um6d+/Otm3bsNtP9THYsmUL0dHRFQo2IvWGxQKRbeHqh+Af8+HhrXD9m9BmMHgFQG4mrJ0JH98CL7aAWX93PM/db3bl5USH+PHyzZ34dtRVJLVsSHGpnelLd9BzwhLeX5VOic1+4RcRkXrH1JabsWPHcu2119KsWTNyc3OZNWsWL774IvPmzaNv374MGzaMJk2aMG7cOAAyMjJo164dw4cP5/7772fr1q3ceeedPPDAAzz22GMVek+13Ei9V1II6T+fatXJ2VN+f+POpxb5jOroCEouwDAMFm3O4oW5m9h+MB+AluEBjL22DcltItTpWKSOc5vbUiNGjGDhwoVkZmYSEhJCx44deeSRR+jbty8APXv2pHnz5sycObPsnFWrVvGvf/2LlJQUmjRpwogRI5yOljqTwo3IaQwDDvxxqp/O3rXl9wc3OXX7qsU14OVrTp2nKbHZmb16N68u2MqR/GIArmwZxuOD2tK+SYjJ1YlITXGbcGMGhRsRJ3IPwNZ5jrCzYzGUFJza5xUAcb1OjL7qD4ER5tUJ5BSWMHXJdt75eSfFpY7bU3/t3ISH+yfQuIFr9iESkapTuHFC4UakgkqOw87lp25f5e47bafFsd7VyckDI9uZdvtqz9ECJsxL4+sUR31eHha6tgijV0IEvRMjaBnuvktViMgpCjdOKNyIVIFhwP7fHSEnbS5kppTfHxJzYpHPAY6lITx9ar3ElIxjvPD9JlanHym3vXlDf3qeCDpXtAzDx9O9FyAVqa8UbpxQuBGpBjmZjhmSt/wIO5ZAaeGpfd6BJ25fXeu4fRXQqNbKMgyDnYfyWbQ5i8VpWazeeYQS26l/4vy9PejeqhG9EiLolRhOdIhuX4m4C4UbJxRuRKpZcQHsXHpiSYh5kHf6cHILxHQ90apzLYQn1urtq9zCElZsO8TizQdZnJZFVm75aSbaRAfTOzGc3okRXBITiodVI65EXJXCjRMKNyI1yG533LI6ucjn/t/L728Qe2qW5Nju4Fl781PZ7QYbM3NYtDmLRZuz2LDnGKf/69fA34serR1B55r4cEIDNHeWiCtRuHFC4UakFmXvPRV0di4D22ktJz7BENfbEXbi+4F/WK2WdjiviKVbDrJocxbLthwkp7C0bJ/VAp2bhdI7MYJeCRG0iQ7SPDoiJlO4cULhRsQkxfmwfbFj9NWWnyA/69Q+ixVirjh1+6pR61q9fVVqs7Nu9zFHX53NWWUrk58UFexLr8RweiVE0L1VIwJ8PGutNhFxULhxQuFGxAXY7bBv3Yl+Oj86JhI8XWgLR8hJuBaaJYFH7a4jtedoAUvSDrJ4cxYrth8qW7wTwNvDyhUtw8padZo3CqjV2kTqK4UbJxRuRFzQsd2OzshpP0D6crAVn9rnEwLxyY7RV/HJ4Bdaq6UVlthYteMwSzZnsSgti4wjx8vtb9kogF6JjqHmlzcPw9vT5ZbsE6kTFG6cULgRcXFFuSduX/3oCDwFh07ts3g4WnJOTh7YqFWtlmYYBtsP5p24fXWQNelHKLWf+ic0wNuDq+IblbXqRASbv1yFSF2hcOOEwo2IG7HbHOtdpc11TCB4cFP5/Q1bneqnE3MleNRuX5icwhJ+3nqIRZuzWJJ2kEN55Yeat28SfGJOnQg6NW2goeYiF0HhxgmFGxE3djT91CKf6SvAXnJqn28DiO8LrfpCRBtoGAfetdcfxm43+GNfdlmn5A17ssvtDwvwpkfrcHolRtAjPpwQ/9rtRyTi7hRunFC4EakjCnNg+0JH2Nn6Exw/cvYxwU0crTsNW0GjeGgY7wg9DZqBtWaXYTiY6xhqvvjEUPPcolNDzT2sFi5rFkqvRMdMyQmRGmouciEKN04o3IjUQXYbZKx2tOjs/gUOb4OCw+c/3sMHwlo6gk6j+BMBKN7xcw3Mt1Nis7N211EWn5hAcGtWXrn9TRr40TPBMYFgt7hG+Hlr/SuRMyncOKFwI1JPFByBw9vh8FY4tNUReA5vc2yzFZ3/PL/QU0GnYdypn0NbgFf1dBDOOFLA4jTH7auV2w9TVHraUHNPK0ktG9L7xAismDD/anlPEXencOOEwo1IPWe3QfaeE6HnZOA58XPOnvOfZ7E6Vj8va+lpdern4CZVnnTweLGNVTsOlY3A2nus/FDzuPAAx+irE0PNvTw01FzqJ4UbJxRuROS8igvgyPYTLT1ntPoU5Zz/PC//E608rc5u9fGt+L8zhmGwNSuvbP2rtbuOYjttqHmQjydXxTeiV2IEPRPCiQjSUHOpPxRunFC4EZFKMwzIP3ja7a3TWn2O7gR76fnPDYw8u6WnYTyExl5w5uXsghKWb3Osf7U07SCH84vL7e/YNKRsqHnHJiFYNdRc6jCFGycUbkSkWtlK4Oiu00LPaa0+eQfOf57VE0Kbn2jpaVW+1Scg/KzbXHa7we97Tw01T91bfqh5o0BverR29NO5Kr4RIX4aai51i8KNEwo3IlJrCrNPBJ1t5Vt9Dm+HkoLzn+cTfEZLz2kPb0cH46ycQsf6V2lZLN96iLzThpp7Wi1cFhta1im5VUSghpqL21O4cULhRkRMZ7dDbubZLT2HtjrW2cLJP8vBTU8bwu4IP8UN4vjtqD+LthxmcVoW2w/mlzulaagfvRIcQScpriG+XhpqLu5H4cYJhRsRcWklhY5+PKcPXz/587kmKjzp5Nw9jVqR7d+c1MJGLDrUgG/3BnCw9NRwcl8vK93iGtErwTFbctNQDTUX96Bw44TCjYi4rYIjZ3RqPtHqc2R7+ZXUz1Ds3YBMzxhSC8P5syiCHUY0241odhuRNI88MVNyQgSXxYZqqLm4LIUbJxRuRKTOsdsgO+PECK6t5Vt7cvae9zSbYWGPEc4OI5qdRjR7PZsS3LQN8YmX0LVTexppqLm4EIUbJxRuRKReKc4/1afn8Mk5fE4MZS/OPe9p+YYP+72aUtqgJQ1i2hLevD3Wk6O6KjF3j0h1UbhxQuFGRATH3D15WWUtPfaDW8nZuwn7wa2EFO7BA/t5T7UHRGBt1PrsIewNYsHDsxYvQuqTynx/62+hiEh9ZLFAUKTj0fwqrECDk/tsJRzKSGNj6joO7PwDDm+lmbGPlpZMwi3ZWPOzID8Ldv1c/jWtno41uM5cl6thq3PO3SNSU9RyIyIiThWV2liz8yiLNmexevMOrEd20NKSSUvrPlpa9pPguZ/mZOJlOFmQ1CfkjJaeEz+HxZXN3SPijG5LOaFwIyJycXYeymfx5iwWp2Xx644jFNvsWLATxVHaeO2nd0QOXQIP05xMfLN3XHjunpCYM1Zgbw5BURAUDf6NwKoRXKJw45TCjYhI9ckvKuXnbYdYkuZY7PNATvnWm8SoIJJbhzAguoA2Xll4HN1WfjX240edv4HV07E+18mwExR1xs/RjodfqG571XEKN04o3IiI1AzDMNiYmcOSNMdin+t3H+W0Rc0J8fPimtbh9E4Mp0frCMICvCH/8Bnz9mxzDGvP3e/o8Oysxed0Hj7nCD3n+K9PkEKQm1K4cULhRkSkdhzNL2bplhOrmm85SPbxkrJ9Fgt0jmlQtqp5u8bBZ69/ZSt1dFzOzXSEnbP+e+LngsMVL8or4AKtQCf+q35ALkfhxgmFGxGR2ldqs5OScYxFmx23rzbvLz/HTmSwD70SIuiZEEG3Vg0J9q3EqualRY4V2E8PPjn7zg5CRdkXfq2TfEJOhZ/gxucOQYGR4OlT8deUi6Jw44TCjYiI+TKzj7N4s6NVZ8W2QxwvsZXb3yzMn7bRwbRtHEybE/9tHOJ7caubF+eXb/E5VytQbqbzFdvP5N/wwq1AARGa/6caKNw4oXAjIuJaCktsrN55hEWbs1iSlkX64XOHixA/L9pEB9E2OoS2jYNpGx1Mq4hAvD2rcTSVYUBR7nkC0Bn/tTkZ+l6OBQIjzhN+TmsV8m+okWFOKNw4oXAjIuLajuYXsykzh42ZOWzc5/jvtqw8Su1nf115eVhoFRFE2+hgR/A5EXoa+HvXbJGG4RjpdbK155ytQCd+NmwXfj04MTIsynkrUFBUvR0ZpnDjhMKNiIj7KSq1sfVAXrnQsykzh5zC0nMe3zjEtyzonLy1FRPqj9Vay6HAboeCQxduBaqJkWF1bA0whRsnFG5EROoGwzDYe+x4WevOyeCTceT4OY8P9PGkTXSQow/PidDTOjIIXy+PWq78HJyNDMs57efjRyr+mt6BFx4ZFhjlNiPDFG6cULgREanbcgpL2JyZy8Z92SdCTy5pB3IpLj17MVCrBeLCA091XD4RehoFuugoqNIiJ52iqzgyzDfkwq1AgVHgWcO3+i5A4cYJhRsRkfqnxGZnx8H8s/ryHMkvPufx4UE+ZUHH0Z8nmBaNAvCo7dtaVVWrI8NO6xQdEF5jI8MUbpxQuBEREXDc1srKLSoLOhszc9i0L4edh/M51zejr5eVxKhTQ9PbRgeTGBVEgI+bDvMuGxl2rk7RZ44MO3cIPIvF6hj6HnM53PJhtZZbme9vN/1ERERELo7FYiEy2JfIYF96JUaUbS8oLmXz/txyfXk2Z+ZyvMRGSsYxUjKOnfYa0LxhwBmjtUKIDPa5uDl5aoPF4uh07BsM4QnnP66yI8Py9sPxY7V2GeeilhsREZELsNkN0g+fuK2179Strazcc891ExbgfWJOnlOjteLCA/HyqMPz2NhtjqUwcjMdz6M7VevL67aUEwo3IiJSXQ7lFZUFnpP9ebYfzMd2jjl5vD2stI4KpE3UaX15GgdXbqmJekzhxgmFGxERqUmFJY45eTZmZp8IPblszMwhr+jcc/I0DfUrv9REdDBNQ/1c/7ZWLVO4cULhRkREapvdbrDn6PGyjssnW3r2Hjv3nDxBvp7lhqa3jQ4mPjIQH08XmJPHJAo3TijciIiIq8guKDk1UutE6NmalUuJ7eyvZk+rhVYRgWVD00+29IQFmDv/TG1RuHFC4UZERFxZcamd7QfzynVc3rQ/h2MFJec8Pir47KUmYsNMWGqihincOKFwIyIi7sYwDDKzC8t1XN6YmcOu86yg7u/tQZuTw9NPrKKeEBmEn7f73tZSuHFC4UZEROqK3MIS0vbnlruttXl/LkXnWWqiRaMA2jYOKTdMPSLI14TKK0/hxgmFGxERqctKbXZ2Hso/q/PyobxzzzLcKNDntAkIHY8WjQLwdLE5eRRunFC4ERGR+igrt7Dc0PSN+7LZeSifc0zJg4+nlcSooPJLTUQHE2jiUhMKN04o3IiIiDgcL7aRdiC3XF+eTZk5FBTbznl8bEP/stadk8EnOsS3VubkUbhxQuFGRETk/Ox2g11HCsotNbEpM4fM7MJzHt/A36vcrMttGzuWmvD2rN7bWgo3TijciIiIVN6R/GI2ndZxeWNmDtuy8ig9x32tlo0CWPRwz2p9f60KLiIiItUqLMCb7q0a0b1Vo7JtRaUnl5oov75Wq4hAEytVuBEREZEq8vH0oH2TENo3CSnbZhgG+efps1NbXGucl4iIiLg1i8Vi6qgqMDncTJ06lY4dOxIcHExwcDBJSUn88MMPFTp39uzZWCwWhgwZUrNFioiIiFsxNdw0bdqU8ePHs3btWn777Td69+7N9ddfz59//un0vPT0dB5++GGuvvrqWqpURERE3IXLjZYKCwtjwoQJjBgx4pz7bTYb11xzDXfeeSfLly/n2LFjfPXVVxV+fY2WEhERcT+V+f52mT43NpuN2bNnk5+fT1JS0nmPe/bZZ4mIiDhv+DlTUVEROTk55R4iIiJSd5k+Wio1NZWkpCQKCwsJDAxkzpw5tG3b9pzH/vzzz7zzzjukpKRU+PXHjRvHM888U03VioiIiKszveUmISGBlJQUfv31V+69916GDx/Oxo0bzzouNzeX22+/nRkzZtCoUaNzvNK5jR07luzs7LJHRkZGdZYvIiIiLsbl+twkJycTFxfH9OnTy21PSUmhc+fOeHh4lG2z2x1LulutVtLS0oiLi7vg66vPjYiIiPtx6xmK7XY7RUVFZ21PTEwkNTW13LbHH3+c3NxcJk2aRExMTG2VKCIiIi7M1HAzduxYrr32Wpo1a0Zubi6zZs1iyZIlzJs3D4Bhw4bRpEkTxo0bh6+vL+3bty93foMGDQDO2i4iIiL1l6nhJisri2HDhpGZmUlISAgdO3Zk3rx59O3bF4Ddu3djtZreLUhERETciMv1ualp6nMjIiLiftxynhsRERGR6qBwIyIiInWKy42Wqmkn78JppmIRERH3cfJ7uyK9aepduMnNzQXQ0HERERE3lJubS0hIiNNj6l2HYrvdzr59+wgKCsJisVTra+fk5BATE0NGRkad7Kxc168P6v416vrcX12/Rl2f+6upazQMg9zcXBo3bnzBkdT1ruXGarXStGnTGn2P4ODgOvuXFur+9UHdv0Zdn/ur69eo63N/NXGNF2qxOUkdikVERKROUbgRERGROkXhphr5+Pjw1FNP4ePjY3YpNaKuXx/U/WvU9bm/un6Nuj735wrXWO86FIuIiEjdppYbERERqVMUbkRERKROUbgRERGROkXhRkREROoUhZtKeuONN2jevDm+vr5cccUVrF692unxn332GYmJifj6+tKhQwfmzp1bS5VWTWWub+bMmVgslnIPX1/fWqy2cpYtW8bgwYNp3LgxFouFr7766oLnLFmyhEsvvRQfHx9atWrFzJkza7zOqqrs9S1ZsuSsz89isbB///7aKbiSxo0bx+WXX05QUBAREREMGTKEtLS0C57nTr+DVblGd/o9nDp1Kh07diyb3C0pKYkffvjB6Tnu9PlV9vrc6bM7l/Hjx2OxWBgzZozT48z4DBVuKuGTTz7hwQcf5KmnnmLdunV06tSJ/v37k5WVdc7jV65cya233sqIESNYv349Q4YMYciQIfzxxx+1XHnFVPb6wDEDZWZmZtlj165dtVhx5eTn59OpUyfeeOONCh2/c+dOBg0aRK9evUhJSWHMmDH84x//YN68eTVcadVU9vpOSktLK/cZRkRE1FCFF2fp0qWMHDmSX375hfnz51NSUkK/fv3Iz88/7znu9jtYlWsE9/k9bNq0KePHj2ft2rX89ttv9O7dm+uvv54///zznMe72+dX2esD9/nszrRmzRqmT59Ox44dnR5n2mdoSIV17drVGDlyZNlzm81mNG7c2Bg3btw5j7/55puNQYMGldt2xRVXGHfffXeN1llVlb2+9957zwgJCaml6qoXYMyZM8fpMf/5z3+Mdu3aldt2yy23GP3796/ByqpHRa5v8eLFBmAcPXq0VmqqbllZWQZgLF269LzHuNvv4Jkqco3u/HtoGIYRGhpqvP322+fc5+6fn2E4vz53/exyc3ON+Ph4Y/78+UaPHj2M0aNHn/dYsz5DtdxUUHFxMWvXriU5Oblsm9VqJTk5mVWrVp3znFWrVpU7HqB///7nPd5MVbk+gLy8PGJjY4mJibng/6G4G3f6/C7GJZdcQnR0NH379mXFihVml1Nh2dnZAISFhZ33GHf/DCtyjeCev4c2m43Zs2eTn59PUlLSOY9x58+vItcH7vnZjRw5kkGDBp312ZyLWZ+hwk0FHTp0CJvNRmRkZLntkZGR5+2jsH///kodb6aqXF9CQgLvvvsuX3/9NR9++CF2u51u3bqxZ8+e2ii5xp3v88vJyeH48eMmVVV9oqOjmTZtGl988QVffPEFMTEx9OzZk3Xr1pld2gXZ7XbGjBlD9+7dad++/XmPc6ffwTNV9Brd7fcwNTWVwMBAfHx8uOeee5gzZw5t27Y957Hu+PlV5vrc7bMDmD17NuvWrWPcuHEVOt6sz7DerQou1ScpKanc/5F069aNNm3aMH36dJ577jkTK5OKSEhIICEhoex5t27d2L59O6+++ioffPCBiZVd2MiRI/njjz/4+eefzS6lxlT0Gt3t9zAhIYGUlBSys7P5/PPPGT58OEuXLj1vAHA3lbk+d/vsMjIyGD16NPPnz3f5js8KNxXUqFEjPDw8OHDgQLntBw4cICoq6pznREVFVep4M1Xl+s7k5eVF586d2bZtW02UWOvO9/kFBwfj5+dnUlU1q2vXri4fGEaNGsV3333HsmXLaNq0qdNj3el38HSVucYzufrvobe3N61atQLgsssuY82aNUyaNInp06efdaw7fn6Vub4zufpnt3btWrKysrj00kvLttlsNpYtW8aUKVMoKirCw8Oj3DlmfYa6LVVB3t7eXHbZZSxcuLBsm91uZ+HChee9n5qUlFTueID58+c7vf9qlqpc35lsNhupqalER0fXVJm1yp0+v+qSkpLisp+fYRiMGjWKOXPmsGjRIlq0aHHBc9ztM6zKNZ7J3X4P7XY7RUVF59znbp/fuTi7vjO5+mfXp08fUlNTSUlJKXt06dKFoUOHkpKSclawARM/wxrtrlzHzJ492/Dx8TFmzpxpbNy40bjrrruMBg0aGPv37zcMwzBuv/1249FHHy07fsWKFYanp6cxceJEY9OmTcZTTz1leHl5GampqWZdglOVvb5nnnnGmDdvnrF9+3Zj7dq1xt///nfD19fX+PPPP826BKdyc3ON9evXG+vXrzcA45VXXjHWr19v7Nq1yzAMw3j00UeN22+/vez4HTt2GP7+/sa///1vY9OmTcYbb7xheHh4GD/++KNZl+BUZa/v1VdfNb766itj69atRmpqqjF69GjDarUaCxYsMOsSnLr33nuNkJAQY8mSJUZmZmbZo6CgoOwYd/8drMo1utPv4aOPPmosXbrU2Llzp/H7778bjz76qGGxWIyffvrJMAz3//wqe33u9Nmdz5mjpVzlM1S4qaTXX3/daNasmeHt7W107drV+OWXX8r29ejRwxg+fHi54z/99FOjdevWhre3t9GuXTvj+++/r+WKK6cy1zdmzJiyYyMjI42BAwca69atM6Hqijk59PnMx8lrGj58uNGjR4+zzrnkkksMb29vo2XLlsZ7771X63VXVGWv78UXXzTi4uIMX19fIywszOjZs6exaNEic4qvgHNdG1DuM3H338GqXKM7/R7eeeedRmxsrOHt7W2Eh4cbffr0KfviNwz3//wqe33u9Nmdz5nhxlU+Q4thGEbNtg2JiIiI1B71uREREZE6ReFGRERE6hSFGxEREalTFG5ERESkTlG4ERERkTpF4UZERETqFIUbERERqVMUbkRERKROUbgRkXrPYrHw1VdfmV2GiFQThRsRMdUdd9yBxWI56zFgwACzSxMRN+VpdgEiIgMGDOC9994rt83Hx8ekakTE3anlRkRM5+PjQ1RUVLlHaGgo4LhlNHXqVK699lr8/Pxo2bIln3/+ebnzU1NT6d27N35+fjRs2JC77rqLvLy8cse8++67tGvXDh8fH6Kjoxk1alS5/YcOHeKGG27A39+f+Ph4vvnmm5q9aBGpMQo3IuLynnjiCW688UY2bNjA0KFD+fvf/86mTZsAyM/Pp3///oSGhrJmzRo+++wzFixYUC68TJ06lZEjR3LXXXeRmprKN998Q6tWrcq9xzPPPMPNN9/M77//zsCBAxk6dChHjhyp1esUkWpS4+uOi4g4MXz4cMPDw8MICAgo93j++ecNwzAMwLjnnnvKnXPFFVcY9957r2EYhvHWW28ZoaGhRl5eXtn+77//3rBarcb+/fsNwzCMxo0bG4899th5awCMxx9/vOx5Xl6eARg//PBDtV2niNQe9bkREdP16tWLqVOnltsWFhZW9nNSUlK5fUlJSaSkpACwadMmOnXqREBAQNn+7t27Y7fbSUtLw2KxsG/fPvr06eO0ho4dO5b9HBAQQHBwMFlZWVW9JBExkcKNiJguICDgrNtE1cXPz69Cx3l5eZV7brFYsNvtNVGSiNQw9bkREZf3yy+/nPW8TZs2ALRp04YNGzaQn59ftn/FihVYrVYSEhIICgqiefPmLFy4sFZrFhHzqOVGRExXVFTE/v37y23z9PSkUaNGAHz22Wd06dKFq666io8++ojVq1fzzjvvADB06FCeeuophg8fztNPP83Bgwe5//77uf3224mMjATg6aef5p577iEiIoJrr72W3NxcVqxYwf3331+7FyoitULhRkRM9+OPPxIdHV1uW0JCAps3bwYcI5lmz57NfffdR3R0NB9//DFt27YFwN/fn3nz5jF69Gguv/xy/P39ufHGG3nllVfKXmv48OEUFhby6quv8vDDD9OoUSNuuumm2rtAEalVFsMwDLOLEBE5H4vFwpw5cxgyZIjZpYiIm1CfGxEREalTFG5ERESkTlGfGxFxabpzLiKVpZYbERERqVMUbkRERKROUbgRERGROkXhRkREROoUhRsRERGpUxRuREREpE5RuBEREZE6ReFGRERE6pT/D3belqU9+/8AAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cpu.\n",
      "On device cpu.\n",
      "Training Accuracy:     0.3094\n",
      "Validation Accuracy:   0.2964\n",
      "On device cpu.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()                             \n\u001b[1;32m     32\u001b[0m n_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[0;32m---> 34\u001b[0m train(n_epochs, optimizer, model, loss_fn, train_loader_embedding, val_loader_embedding)\n\u001b[1;32m     36\u001b[0m acc_train \u001b[39m=\u001b[39m compute_accuracy(model, train_loader_embedding)\n\u001b[1;32m     37\u001b[0m acc_val \u001b[39m=\u001b[39m compute_accuracy(model, val_loader_embedding)\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     loss_train \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mfor\u001b[39;00m contexts, targets \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     27\u001b[0m         contexts \u001b[39m=\u001b[39m contexts\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     28\u001b[0m         targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/INF265/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/INF265/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/INF265/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/INF265/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/INF265/lib/python3.10/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(tensor[index] \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/INF265/lib/python3.10/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(tensor[index] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "device = set_device()\n",
    "\n",
    "batch_size=512\n",
    "train_loader_embedding = DataLoader(data_train_embedding, batch_size=batch_size, shuffle=True)\n",
    "val_loader_embedding = DataLoader(data_val_embedding, batch_size=batch_size, shuffle=True)\n",
    "test_loader_embedding = DataLoader(data_test_embedding, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "embedding_model_1 = MyEmbedding1(10)\n",
    "embedding_model_2 = MyEmbedding2(10)\n",
    "embedding_model_3 = MyEmbedding3(10)\n",
    "\n",
    "embedding_models = [embedding_model_1,embedding_model_2, embedding_model_3]\n",
    "\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + 'best_embedding_model.pt'):\n",
    "    # Load the trained models\n",
    "    best_embedding_model = torch.load(PATH_GENERATED + 'best_embedding_model.pt')\n",
    "    best_embedding_model.to(device)\n",
    "else:\n",
    "    best_embedding_model = None\n",
    "    best_embedding_acc = 0 \n",
    "\n",
    "    num_trained = 0\n",
    "    for model in embedding_models:\n",
    "        num_trained +=1 \n",
    "        MODEL_FNAME = f'embedding_{num_trained}.pt'\n",
    "\n",
    "        model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()                             \n",
    "        n_epochs=5\n",
    "\n",
    "        train(n_epochs, optimizer, model, loss_fn, train_loader_embedding, val_loader_embedding)\n",
    "\n",
    "        acc_train = compute_accuracy(model, train_loader_embedding)\n",
    "        acc_val = compute_accuracy(model, val_loader_embedding)\n",
    "\n",
    "        if best_embedding_acc < acc_val: \n",
    "            best_embedding_model = model \n",
    "        \n",
    "        print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "        print(\"Validation Accuracy:   %.4f\" %acc_val)\n",
    "\n",
    "    torch.save(best_embedding_model.to(device=\"cpu\"), PATH_GENERATED + 'best_embedding_model.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'head':\n",
      "['head', 'eyes', 'lips', 'forehead', 'face', 'arms', 'hair', 'chest', 'teeth', 'arm']\n",
      "Most similar words to 'you':\n",
      "['you', 'ye', 'i', 'we', 'thee', 'myself', 'us', 'everybody', 'me', 'ourselves']\n",
      "Most similar words to 'boy':\n",
      "['boy', 'fellow', 'instinct', 'maid', 'devil', 'doctor', 'wife', 'law', 'cat', 'bishop']\n",
      "Most similar words to 'no':\n",
      "['no', 'any', 'become', 'according', 'sufficient', 'been', 'different', 'increased', 'existed', 'such']\n",
      "Most similar words to 'child':\n",
      "['child', 'lad', 'fellow', 'family', 'peasant', 'creature', 'stranger', 'society', 'woman', 'convict']\n",
      "Most similar words to 'yes':\n",
      "['yes', 'to-day', 'all', 'sir', 'none', 'tomorrow', 'well', 'abruptly', 'sure', 'why']\n",
      "Most similar words to 'chair':\n",
      "['chair', 'shield', 'tomb', 'height', 'gun', 'duty', 'youth', 'nose', 'kinds', 'hat']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cos_sin(weights): \n",
    "    similarity_matrix = []\n",
    "    for i in range(len(weights)):\n",
    "        similarity_matrix.append(F.cosine_similarity(weights[i], weights, dim=1))\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Load trained embedding model and vocabulary\n",
    "model_path = 'generated/model.pt'\n",
    "vocab_path = \"generated/vocabulary.pt\"\n",
    "model = torch.load(model_path)\n",
    "vocab = torch.load(vocab_path)\n",
    "\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "weights = model.embedding.weight\n",
    "similarity_matrix = cos_sin(weights)\n",
    "\n",
    "sample_words = ['head', 'you', 'boy', 'no', 'child', 'school', 'yes', 'chair']\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "for word in sample_words:\n",
    "\n",
    "    if word in vocab:\n",
    "\n",
    "        idx = vocab[word]\n",
    "        sim_scores = similarity_matrix[idx]\n",
    "        sim_vals, sim_indices = torch.topk(sim_scores, 10)\n",
    "        #sim_indices = np.argsort(sim_scores)[:top_k]\n",
    "\n",
    "        sim_words = [vocab.lookup_token(i) for i in sim_indices]\n",
    "        print(\"Most similar words to '%s':\" % word)\n",
    "        print(sim_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame(vocab.lookup_tokens([i for i in range(len(vocab))]))\n",
    "vocab_df.to_csv('vocab_df.tsv', sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "embedding_df = pd.DataFrame(model.embedding.weight.data.numpy())\n",
    "embedding_df.to_csv('embedding_df.tsv', sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Define targets ------------------------------\n",
    "def compute_label_conjugation(w):\n",
    "    \"\"\"\n",
    "    helper function to define MAP_TARGET\n",
    "    \n",
    "    - 0 = 'unknown word'\n",
    "    - 1 = 'punctuation' (i.e. the '<unk>' token)\n",
    "    - 2 = 'is an actual word'\n",
    "    \"\"\"\n",
    "    if w in ['be', 'am', 'are', 'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# true labels for this task:\n",
    "MAP_TARGET = {}\n",
    "i = 0\n",
    "for word in ['be', 'am', 'are', 'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having']: \n",
    "    MAP_TARGET[word] = i\n",
    "    i += 1\n",
    "\n",
    "# context size for this task \n",
    "CONTEXT_SIZE = 3\n",
    "\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def create_dataset_conjugation(\n",
    "    text, vocab, \n",
    "    context_size=CONTEXT_SIZE, map_target=MAP_TARGET\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    n_text = len(text)\n",
    "    n_vocab = len(vocab)\n",
    "    \n",
    "    # Change labels if only a few target are kept, otherwise, each word is\n",
    "    # associated with its index in the vocabulary\n",
    "    if map_target is None:\n",
    "        map_target = {i:i for i in range(n_vocab)}\n",
    "    \n",
    "    # Transform the text as a list of integers.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    # Start constructing the context / target pairs...\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - 2*context_size):\n",
    "        \n",
    "        # Word used to define target\n",
    "        t = txt[i + context_size]\n",
    "        \n",
    "        # Context before the target and after the target. \n",
    "        c = txt[i: i+context_size]\n",
    "        c += txt[i+context_size+1 : i + (2*context_size)+1]\n",
    "        \n",
    "        if compute_label_conjugation(vocab.lookup_token(t)) == 0:\n",
    "            targets.append(map_target[vocab.lookup_token(t)])\n",
    "            contexts.append(torch.tensor(c))\n",
    "            \n",
    "    # contexts of shape (N_dataset, context_size)\n",
    "    # targets of shape  (N_dataset)\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    # Create a pytorch dataset out of these context / target pairs\n",
    "\n",
    "    print(len(contexts), len(targets))\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "def load_dataset_conjugation(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    # If already generated\n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        # Create context / target dataset based on the list of strings\n",
    "        dataset = create_dataset_conjugation(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train_conjugation = load_dataset_conjugation(words_train, vocab, \"data_train_conjugation.pt\")\n",
    "data_val_conjugation = load_dataset_conjugation(words_val, vocab, \"data_val_conjugation.pt\")\n",
    "data_test_conjugation = load_dataset_conjugation(words_test, vocab, \"data_test_conjugation.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_matrix, max_len):\n",
    "        super(MLP, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.fc1 = nn.Linear(max_len * embedding_matrix.shape[1], 256)                                       \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 12)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        x = embeddings.view(embeddings.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "import math\n",
    "\n",
    "class MLPWithAttention(nn.Module):\n",
    "    def __init__(self, embedding_matrix, emb_dim, max_len, num_classes=12):\n",
    "        super(MLPWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.fc1 = nn.Linear(emb_dim * max_len, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.positional_encoding = self.positional_encoding(max_len, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).view(x.shape[0], -1)\n",
    "        if self.positional_encoding is not None:\n",
    "            embedded = embedded + self.positional_encoding\n",
    "        x = self.fc1(embedded)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def positional_encoding(self, max_len, emb_dim):\n",
    "        pe = torch.zeros(max_len, emb_dim)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        pe = pe.to(device)\n",
    "        for i in range(max_len):\n",
    "            for j in range(0, emb_dim, 2):\n",
    "                k = j // 2\n",
    "                pe[i, j] = math.sin(i / (10000 ** (k / emb_dim)))\n",
    "                pe[i, j + 1] = math.cos(i / (10000 ** (k / emb_dim)))\n",
    "        return pe.flatten()\n",
    "    \n",
    "class RNN_conjugation(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, num_classes=11):\n",
    "        super(RNN_conjugation, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.rnn = nn.LSTM(embedding_matrix.shape[1], hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, c_n) = self.rnn(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cpu.\n",
      "On device cpu.\n",
      "13:39:24.513248  |  Epoch 1  |  Training loss 1.47381\n",
      "13:39:25.916884  |  Epoch 2  |  Training loss 1.22579\n",
      "13:39:27.355995  |  Epoch 3  |  Training loss 1.15505\n",
      "13:39:28.641181  |  Epoch 4  |  Training loss 1.11241\n",
      "13:39:30.167310  |  Epoch 5  |  Training loss 1.08258\n",
      "On device cpu.\n",
      "On device cpu.\n",
      "Training Accuracy:     0.6022\n",
      "Validation Accuracy:   0.5923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(seed)\n",
    "device = set_device()\n",
    "\n",
    "\n",
    "MODEL_FNAME = \"model_conjugation.pt\"\n",
    "\n",
    "batch_size=512\n",
    "train_loader_conjugation = DataLoader(data_train_conjugation, batch_size=batch_size, shuffle=True)\n",
    "val_loader_conjugation = DataLoader(data_val_conjugation, batch_size=batch_size, shuffle=True)\n",
    "test_loader_conjugation = DataLoader(data_test_conjugation, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mlp_model = MLP(model.embedding.weight.data, 2*CONTEXT_SIZE) #2*contex_size fordi det er 3 ord på hver side. \n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + MODEL_FNAME):\n",
    "    # Load the trained model\n",
    "    mlp_model = torch.load(PATH_GENERATED + MODEL_FNAME)\n",
    "    mlp_model.to(device)\n",
    "else:\n",
    "    # Or train the model...\n",
    "    mlp_model.to(device)\n",
    "    optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()                             #Add weights\n",
    "    n_epochs=5\n",
    "\n",
    "    train(n_epochs, optimizer, mlp_model, loss_fn, train_loader_conjugation)\n",
    "    # ... and save it\n",
    "    torch.save(mlp_model.to(device=\"cpu\"), PATH_GENERATED + MODEL_FNAME)\n",
    "\n",
    "acc_train = compute_accuracy(mlp_model, train_loader_conjugation)\n",
    "acc_val = compute_accuracy(mlp_model, val_loader_conjugation)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cpu.\n",
      "On device cpu.\n",
      "13:40:05.451126  |  Epoch 1  |  Training loss 1.66255\n",
      "13:40:06.509422  |  Epoch 2  |  Training loss 1.34991\n",
      "13:40:07.575759  |  Epoch 3  |  Training loss 1.27999\n",
      "13:40:08.660360  |  Epoch 4  |  Training loss 1.23433\n",
      "13:40:09.931982  |  Epoch 5  |  Training loss 1.20363\n",
      "On device cpu.\n",
      "On device cpu.\n",
      "Training Accuracy:     0.5575\n",
      "Validation Accuracy:   0.5722\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "device = set_device()\n",
    "\n",
    "\n",
    "MODEL_FNAME = \"model_conjugation_with_attention.pt\"\n",
    "\n",
    "batch_size=512\n",
    "train_loader_conjugation = DataLoader(data_train_conjugation, batch_size=batch_size, shuffle=True)\n",
    "val_loader_conjugation = DataLoader(data_val_conjugation, batch_size=batch_size, shuffle=True)\n",
    "test_loader_conjugation = DataLoader(data_test_conjugation, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "mlp_with_attention_model = MLPWithAttention(embedding_matrix=model.embedding.weight.data, emb_dim=10, max_len=CONTEXT_SIZE*2)\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + MODEL_FNAME):\n",
    "    # Load the trained models\n",
    "    mlp_with_attention_model = torch.load(PATH_GENERATED + MODEL_FNAME)\n",
    "    mlp_with_attention_model.to(device)\n",
    "else:\n",
    "    # Or train the model...\n",
    "    mlp_with_attention_model.to(device)\n",
    "    optimizer = optim.Adam(mlp_with_attention_model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()                             #Add weights\n",
    "    n_epochs=10\n",
    "\n",
    "    train(n_epochs, optimizer, mlp_with_attention_model, loss_fn, train_loader_conjugation)\n",
    "    # ... and save it\n",
    "    torch.save(mlp_with_attention_model.to(device=\"cpu\"), PATH_GENERATED + MODEL_FNAME)\n",
    "\n",
    "acc_train = compute_accuracy(mlp_with_attention_model, train_loader_conjugation)\n",
    "acc_val = compute_accuracy(mlp_with_attention_model, val_loader_conjugation)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cpu.\n",
      "On device cpu.\n",
      "13:40:18.257339  |  Epoch 1  |  Training loss 2.12639\n",
      "13:40:19.834686  |  Epoch 2  |  Training loss 1.66584\n",
      "13:40:21.562306  |  Epoch 3  |  Training loss 1.49987\n",
      "13:40:23.131431  |  Epoch 4  |  Training loss 1.43099\n",
      "13:40:24.698232  |  Epoch 5  |  Training loss 1.38875\n",
      "13:40:26.275827  |  Epoch 6  |  Training loss 1.35797\n",
      "13:40:28.054537  |  Epoch 7  |  Training loss 1.33337\n",
      "13:40:29.881021  |  Epoch 8  |  Training loss 1.31236\n",
      "13:40:32.031675  |  Epoch 9  |  Training loss 1.29736\n",
      "13:40:33.862995  |  Epoch 10  |  Training loss 1.28453\n",
      "On device cpu.\n",
      "On device cpu.\n",
      "Training Accuracy:     0.5296\n",
      "Validation Accuracy:   0.5266\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "device = set_device()\n",
    "\n",
    "\n",
    "MODEL_FNAME = \"model_conjugation_RNN.pt\"\n",
    "\n",
    "batch_size=256\n",
    "train_loader_conjugation = DataLoader(data_train_conjugation, batch_size=batch_size, shuffle=True)\n",
    "val_loader_conjugation = DataLoader(data_val_conjugation, batch_size=batch_size, shuffle=True)\n",
    "test_loader_conjugation = DataLoader(data_test_conjugation, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "rnn_conjugation = RNN_conjugation(model.embedding.weight.data, 10, 4)\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + MODEL_FNAME):\n",
    "    # Load the trained models\n",
    "    rnn_conjugation = torch.load(PATH_GENERATED + MODEL_FNAME)\n",
    "    rnn_conjugation.to(device)\n",
    "else:\n",
    "    # Or train the model...\n",
    "    rnn_conjugation.to(device)\n",
    "    optimizer = optim.Adam(rnn_conjugation.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()                             #Add weights\n",
    "    n_epochs=10\n",
    "\n",
    "    train(n_epochs, optimizer, rnn_conjugation, loss_fn, train_loader_conjugation)\n",
    "    # ... and save it\n",
    "    torch.save(rnn_conjugation.to(device=\"cpu\"), PATH_GENERATED + MODEL_FNAME)\n",
    "\n",
    "acc_train = compute_accuracy(rnn_conjugation, train_loader_conjugation)\n",
    "acc_val = compute_accuracy(rnn_conjugation, val_loader_conjugation)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# true labels for this task:\n",
    "MAP_TARGET = {\n",
    "    vocab[w]:compute_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))\n",
    "}\n",
    "\n",
    "# context size for this task \n",
    "CONTEXT_SIZE = 3\n",
    "\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def create_dataset_generation(\n",
    "    text, vocab, \n",
    "    context_size=CONTEXT_SIZE, map_target=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    n_text = len(text)\n",
    "    n_vocab = len(vocab)\n",
    "    \n",
    "    # Change labels if only a few target are kept, otherwise, each word is\n",
    "    # associated with its index in the vocabulary\n",
    "    if map_target is None:\n",
    "        map_target = {i:i for i in range(n_vocab)}\n",
    "    \n",
    "    # Transform the text as a list of integers.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    # Start constructing the context / target pairs...\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        # Word used to define target\n",
    "        t = txt[i + context_size]\n",
    "        \n",
    "        # Context before the target. \n",
    "        c = txt[i: i+context_size]\n",
    "        \n",
    "        if compute_label(vocab.lookup_token(t)) == 2:\n",
    "            targets.append(map_target[t])\n",
    "            contexts.append(torch.tensor(c))\n",
    "            \n",
    "    # contexts of shape (N_dataset, context_size)\n",
    "    # targets of shape  (N_dataset)\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    # Create a pytorch dataset out of these context / target pairs\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    # If already generated\n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        # Create context / target dataset based on the list of strings\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train_generation = load_dataset(words_train, vocab, \"data_train_generation.pt\")\n",
    "data_val_generation  = load_dataset(words_val, vocab, \"data_val_generation.pt\")\n",
    "data_test_generation  = load_dataset(words_test, vocab, \"data_tes_generationt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_generation(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size):\n",
    "        super(RNN_generation, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.rnn = nn.LSTM(embedding_matrix.shape[1], hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, c_n) = self.rnn(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cpu.\n",
      "On device cpu.\n",
      "13:07:08.990229  |  Epoch 1  |  Training loss 5.73254\n",
      "13:07:53.338535  |  Epoch 2  |  Training loss 5.25246\n",
      "13:08:37.253279  |  Epoch 3  |  Training loss 5.06019\n",
      "13:09:24.087912  |  Epoch 4  |  Training loss 5.00705\n",
      "13:16:27.723742  |  Epoch 5  |  Training loss 4.98174\n",
      "13:17:16.579717  |  Epoch 6  |  Training loss 4.96604\n",
      "13:18:02.922742  |  Epoch 7  |  Training loss 4.95492\n",
      "13:18:48.408042  |  Epoch 8  |  Training loss 4.94582\n",
      "13:20:04.048463  |  Epoch 9  |  Training loss 4.93814\n",
      "13:20:55.519463  |  Epoch 10  |  Training loss 4.93107\n",
      "On device cpu.\n",
      "On device cpu.\n",
      "Training Accuracy:     0.1359\n",
      "Validation Accuracy:   0.1316\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "device = set_device()\n",
    "\n",
    "\n",
    "MODEL_FNAME = \"model_generation_RNN.pt\"\n",
    "\n",
    "batch_size=256\n",
    "train_loader_generation = DataLoader(data_train_generation, batch_size=batch_size, shuffle=True)\n",
    "val_loader_generation = DataLoader(data_val_generation, batch_size=batch_size, shuffle=True)\n",
    "test_loader_generation = DataLoader(data_test_generation, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "rnn_generation = RNN_generation(model.embedding.weight.data, 4)\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + MODEL_FNAME):\n",
    "    # Load the trained models\n",
    "    rnn_generation = torch.load(PATH_GENERATED + MODEL_FNAME)\n",
    "    rnn_generation.to(device)\n",
    "else:\n",
    "    # Or train the model...\n",
    "    rnn_generation.to(device)\n",
    "    optimizer = optim.Adam(rnn_generation.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()                             #Add weights\n",
    "    n_epochs=10\n",
    "\n",
    "    train(n_epochs, optimizer, rnn_generation, loss_fn, train_loader_generation)\n",
    "    # ... and save it\n",
    "    torch.save(rnn_generation.to(device=\"cpu\"), PATH_GENERATED + MODEL_FNAME)\n",
    "\n",
    "acc_train = compute_accuracy(rnn_generation, train_loader_generation)\n",
    "acc_val = compute_accuracy(rnn_generation, val_loader_generation)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
